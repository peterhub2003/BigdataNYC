{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Big data Test Notebook\n",
    "\n",
    "This notebook demonstrates how to connect to Spark and Hive, load data, and perform analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Spark Session with Hive Support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/bitnami/spark\n",
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('SPARK_HOME'))\n",
    "os.environ['SPARK_HOME'] = '/usr/local/spark'\n",
    "print(os.environ.get('SPARK_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped existing SparkContext\n",
      "Spark version: 3.3.0\n",
      "Spark UI: http://jupyter:4040\n"
     ]
    }
   ],
   "source": [
    "# First, stop any existing SparkContext\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sc.stop()\n",
    "    print(\"Stopped existing SparkContext\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing SparkContext to stop or error occurred: {e}\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, explode, lit, array\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# Create a Spark session with explicit cluster configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Explicit Spark Job Test\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.driver.host\", \"jupyter\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.driver.memory\", \"1g\") \\\n",
    "    .config(\"spark.yarn.am.memory\", \"1g\") \\\n",
    "    .config(\"spark.yarn.am.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"1g\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.default.parallelism\", \"10\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating large dataset...\n",
      "Caching dataset...\n",
      "Dataset count: 1000000\n",
      "Running aggregation job...\n",
      "Collecting results...\n",
      "Result size: 100\n",
      "Running join operation...\n",
      "Sample of joined data:\n",
      "+---+-------------------+------------+--------------------+---+\n",
      "| id|       random_value|double_value|           array_col| id|\n",
      "+---+-------------------+------------+--------------------+---+\n",
      "|  0| 0.8990252057024203|           0|[0, 1, 2, 3, 4, 5...|  0|\n",
      "|  1| 0.6316103078771831|           2|[0, 1, 2, 3, 4, 5...|  1|\n",
      "|  2|0.26705630323001883|           4|[0, 1, 2, 3, 4, 5...|  2|\n",
      "|  3|0.38013733978644937|           6|[0, 1, 2, 3, 4, 5...|  3|\n",
      "|  4| 0.6930249579784798|           8|[0, 1, 2, 3, 4, 5...|  4|\n",
      "+---+-------------------+------------+--------------------+---+\n",
      "only showing top 5 rows\n",
      "\n",
      "Job completed. Sleeping for 30 seconds so you can check the Spark UI...\n",
      "Stopping Spark session...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a large dataset to force distributed processing\n",
    "print(\"Creating large dataset...\")\n",
    "df = spark.range(0, 1000000, 1, 10)  # 1 million rows with 10 partitions\n",
    "\n",
    "# Add more columns to make the dataset larger\n",
    "df = df.withColumn(\"random_value\", rand()) \\\n",
    "       .withColumn(\"double_value\", col(\"id\") * 2) \\\n",
    "       .withColumn(\"array_col\", array([lit(i) for i in range(10)]))\n",
    "\n",
    "# Force materialization with cache\n",
    "print(\"Caching dataset...\")\n",
    "df.cache()\n",
    "\n",
    "# Force evaluation with count\n",
    "print(f\"Dataset count: {df.count()}\")\n",
    "\n",
    "# Perform a complex operation that will definitely use the cluster\n",
    "print(\"Running aggregation job...\")\n",
    "result = df.groupBy(df.id % 100).count()\n",
    "\n",
    "# Force shuffle with repartition\n",
    "result = result.repartition(20)\n",
    "\n",
    "# Collect results to trigger job execution\n",
    "print(\"Collecting results...\")\n",
    "collected = result.collect()\n",
    "print(f\"Result size: {len(collected)}\")\n",
    "\n",
    "# Perform a join operation (this will trigger another job)\n",
    "print(\"Running join operation...\")\n",
    "df2 = spark.range(0, 100, 1, 5)\n",
    "joined = df.join(df2, df.id % 100 == df2.id, \"inner\")\n",
    "\n",
    "# Force execution with show\n",
    "print(\"Sample of joined data:\")\n",
    "joined.show(5)\n",
    "\n",
    "# Sleep to keep the application alive so you can check the UI\n",
    "print(\"Job completed. Sleeping for 30 seconds so you can check the Spark UI...\")\n",
    "time.sleep(30)\n",
    "\n",
    "# Stop the Spark session\n",
    "print(\"Stopping Spark session...\")\n",
    "spark.stop()\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a Spark session with explicit cluster configuration\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplicit Spark Job Test\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myarn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.host\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjupyter\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.submit.deployMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclient\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.yarn.am.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.yarn.am.cores\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.memory\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1g\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.executor.cores\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.default.parallelism\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.shuffle.partitions\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m10\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;241m.\u001b[39menableHiveSupport() \\\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpark UI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39muiWebUrl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "# # Create a Spark session with explicit cluster configuration\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"Explicit Spark Job Test\") \\\n",
    "#     .master(\"yarn\") \\\n",
    "#     .config(\"spark.driver.host\", \"jupyter\") \\\n",
    "#     .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "#     .config(\"spark.driver.memory\", \"1g\") \\\n",
    "#     .config(\"spark.yarn.am.memory\", \"1g\") \\\n",
    "#     .config(\"spark.yarn.am.cores\", \"1\") \\\n",
    "#     .config(\"spark.executor.memory\", \"1g\") \\\n",
    "#     .config(\"spark.executor.cores\", \"1\") \\\n",
    "#     .config(\"spark.default.parallelism\", \"10\") \\\n",
    "#     .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# print(f\"Spark version: {spark.version}\")\n",
    "# print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date, count\n",
    "import os\n",
    "\n",
    "df = spark.read.parquet(\"hdfs://namenode:9000/data/raw/nyc_trip_data/*.parquet\")\n",
    "\n",
    "df_processed = df.withColumn(\"date\", to_date(col(\"tpep_pickup_datetime\"))) \\\n",
    "                 .groupBy(\"date\") \\\n",
    "                 .agg(count(\"*\").alias(\"num_trips\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|Airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "|       2| 2024-01-01 00:57:55|  2024-01-01 01:17:43|              1|         1.72|         1|                 N|         186|          79|           2|       17.7|  1.0|    0.5|       0.0|         0.0|                  1.0|        22.7|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:03:00|  2024-01-01 00:09:36|              1|          1.8|         1|                 N|         140|         236|           1|       10.0|  3.5|    0.5|      3.75|         0.0|                  1.0|       18.75|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:17:06|  2024-01-01 00:35:01|              1|          4.7|         1|                 N|         236|          79|           1|       23.3|  3.5|    0.5|       3.0|         0.0|                  1.0|        31.3|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:36:38|  2024-01-01 00:44:56|              1|          1.4|         1|                 N|          79|         211|           1|       10.0|  3.5|    0.5|       2.0|         0.0|                  1.0|        17.0|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:46:51|  2024-01-01 00:52:57|              1|          0.8|         1|                 N|         211|         148|           1|        7.9|  3.5|    0.5|       3.2|         0.0|                  1.0|        16.1|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:54:08|  2024-01-01 01:26:31|              1|          4.7|         1|                 N|         148|         141|           1|       29.6|  3.5|    0.5|       6.9|         0.0|                  1.0|        41.5|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:49:44|  2024-01-01 01:15:47|              2|        10.82|         1|                 N|         138|         181|           1|       45.7|  6.0|    0.5|      10.0|         0.0|                  1.0|       64.95|                 0.0|       1.75|\n",
      "|       1| 2024-01-01 00:30:40|  2024-01-01 00:58:40|              0|          3.0|         1|                 N|         246|         231|           2|       25.4|  3.5|    0.5|       0.0|         0.0|                  1.0|        30.4|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:26:01|  2024-01-01 00:54:12|              1|         5.44|         1|                 N|         161|         261|           2|       31.0|  1.0|    0.5|       0.0|         0.0|                  1.0|        36.0|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:28:08|  2024-01-01 00:29:16|              1|         0.04|         1|                 N|         113|         113|           2|        3.0|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:35:22|  2024-01-01 00:41:41|              2|         0.75|         1|                 N|         107|         137|           1|        7.9|  1.0|    0.5|       0.0|         0.0|                  1.0|        12.9|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:25:00|  2024-01-01 00:34:03|              2|          1.2|         1|                 N|         158|         246|           1|       14.9|  3.5|    0.5|      3.95|         0.0|                  1.0|       23.85|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:35:16|  2024-01-01 01:11:52|              2|          8.2|         1|                 N|         246|         190|           1|       59.0|  3.5|    0.5|     14.15|        6.94|                  1.0|       85.09|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:43:27|  2024-01-01 00:47:11|              2|          0.4|         1|                 N|          68|          90|           1|        5.8|  3.5|    0.5|      1.25|         0.0|                  1.0|       12.05|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:51:53|  2024-01-01 00:55:43|              1|          0.8|         1|                 N|          90|          68|           2|        6.5|  3.5|    0.5|       0.0|         0.0|                  1.0|        11.5|                 2.5|        0.0|\n",
      "|       1| 2024-01-01 00:50:09|  2024-01-01 01:03:57|              1|          5.0|         1|                 N|         132|         216|           2|       21.2| 2.75|    0.5|       0.0|         0.0|                  1.0|       25.45|                 0.0|       1.75|\n",
      "|       1| 2024-01-01 00:41:06|  2024-01-01 00:53:42|              1|          1.5|         1|                 N|         164|          79|           1|       12.8|  3.5|    0.5|      4.45|         0.0|                  1.0|       22.25|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:52:09|  2024-01-01 00:52:28|              1|          0.0|         1|                 N|         237|         237|           2|        3.0|  1.0|    0.5|       0.0|         0.0|                  1.0|         8.0|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:56:38|  2024-01-01 01:03:17|              1|          1.5|         1|                 N|         141|         263|           1|        9.3|  1.0|    0.5|       3.0|         0.0|                  1.0|        17.3|                 2.5|        0.0|\n",
      "|       2| 2024-01-01 00:32:34|  2024-01-01 00:49:33|              1|         2.57|         1|                 N|         161|         263|           1|       17.7|  1.0|    0.5|      10.0|         0.0|                  1.0|        32.7|                 2.5|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9554778"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType, DecimalType\n",
    "from pyspark.sql.functions import col, to_date, avg, sum, count\n",
    "\n",
    "# Transform: Aggregate revenue metrics\n",
    "fact_revenue = df.withColumn(\"date\", to_date(col(\"tpep_pickup_datetime\"))) \\\n",
    "    .withColumn(\"payment_type_id\", col(\"Payment_type\").cast(IntegerType())) \\\n",
    "    .groupBy(\"date\", \"payment_type_id\") \\\n",
    "    .agg(\n",
    "        sum(col(\"Fare_amount\").cast(DecimalType(10, 2))).alias(\"total_fare\"),\n",
    "        sum(col(\"Tip_amount\").cast(DecimalType(10, 2))).alias(\"total_tips\"),\n",
    "        avg(col(\"Fare_amount\").cast(DecimalType(10, 2))).alias(\"avg_fare_per_trip\"),\n",
    "        count(\"*\").alias(\"transaction_count\")\n",
    "    )\n",
    "\n",
    "# Dimension table: Payment types (from NYC TLC data dictionary)\n",
    "dim_payment_type_data = [\n",
    "    (1, \"Credit Card\"),\n",
    "    (2, \"Cash\"),\n",
    "    (3, \"No Charge\"),\n",
    "    (4, \"Dispute\"),\n",
    "    (5, \"Unknown\"),\n",
    "    (6, \"Voided Trip\")\n",
    "]\n",
    "\n",
    "dim_payment_type = spark.createDataFrame(dim_payment_type_data, [\"payment_type_id\", \"payment_desc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+\n",
      "|payment_type_id|payment_desc|\n",
      "+---------------+------------+\n",
      "|              1| Credit Card|\n",
      "|              2|        Cash|\n",
      "|              3|   No Charge|\n",
      "|              4|     Dispute|\n",
      "|              5|     Unknown|\n",
      "|              6| Voided Trip|\n",
      "+---------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_payment_type.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+----------+----------+-----------------+-----------------+\n",
      "|      date|payment_type_id|total_fare|total_tips|avg_fare_per_trip|transaction_count|\n",
      "+----------+---------------+----------+----------+-----------------+-----------------+\n",
      "|2024-01-13|              2| 255818.15|     16.68|        16.735454|            15286|\n",
      "|2024-01-13|              3|   3868.72|     21.21|         6.363026|              608|\n",
      "|2024-01-15|              4|   1213.51|     87.45|         1.072005|             1132|\n",
      "|2024-01-19|              1|1341188.40| 304211.09|        17.573454|            76319|\n",
      "|2024-01-23|              3|   3833.31|      0.00|         6.784619|              565|\n",
      "|2024-01-15|              1|1218745.17| 272509.15|        19.892684|            61266|\n",
      "|2024-01-18|              2| 266067.65|     23.78|        17.543693|            15166|\n",
      "|2024-01-22|              4|   1684.57|     71.35|         1.266594|             1330|\n",
      "|2024-01-22|              3|   4157.00|      6.16|         5.655782|              735|\n",
      "|2024-01-23|              2| 240203.78|     18.80|        17.333221|            13858|\n",
      "|2024-01-24|              1|1545485.65| 352837.72|        18.054528|            85601|\n",
      "|2024-01-18|              1|1583672.18| 361202.19|        18.168671|            87165|\n",
      "|2024-01-20|              2| 242216.59|     71.55|        16.726510|            14481|\n",
      "|2024-01-13|              1|1454736.95| 328635.16|        17.652218|            82411|\n",
      "|2024-01-14|              1|1341924.98| 301235.77|        18.262949|            73478|\n",
      "|2024-01-16|              2| 237598.72|     49.11|        18.758781|            12666|\n",
      "|2024-01-17|              2| 267604.07|     73.98|        18.036265|            14837|\n",
      "|2024-01-21|              4|   2088.50|     58.46|         1.402619|             1489|\n",
      "|2024-01-15|              2| 205545.97|     24.00|        18.334312|            11211|\n",
      "|2024-01-16|              3|   3239.18|      6.72|         5.575181|              581|\n",
      "+----------+---------------+----------+----------+-----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_revenue.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the Spark session\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
