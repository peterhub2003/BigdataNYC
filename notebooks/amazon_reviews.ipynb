{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fff8d28-cd40-4b7b-bbc8-7b2513ea1ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/bitnami/spark\n",
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "print(os.environ.get('SPARK_HOME'))\n",
    "os.environ['SPARK_HOME'] = '/usr/local/spark'\n",
    "print(os.environ.get('SPARK_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2182ddaa-440e-4e05-9f13-87bfad9ed739",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark-nlp\n",
      "  Downloading spark_nlp-5.5.3-py2.py3-none-any.whl (635 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m635.7/635.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: spark-nlp\n",
      "Successfully installed spark-nlp-5.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade spark-nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafee7c",
   "metadata": {},
   "source": [
    "## INIT AND DEFINE SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b0f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_spark():\n",
    "    try:\n",
    "        from pyspark import SparkContext\n",
    "        sc = SparkContext.getOrCreate()\n",
    "        sc.stop()\n",
    "        print(\"Stopped existing SparkContext\")\n",
    "    except Exception as e:\n",
    "        print(f\"No existing SparkContext to stop or error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d426f-df48-4163-a277-418401acb147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped existing SparkContext\n"
     ]
    }
   ],
   "source": [
    "# First, stop any existing SparkContext        \n",
    "def init_spark():\n",
    "    stop_spark()\n",
    "    spark_nlp_jar_path = \"/usr/local/spark/jars/spark-nlp_2.12-5.3.3.jar\" \n",
    "    \n",
    "    from pyspark.sql import SparkSession\n",
    "    # Create a Spark session with explicit cluster configuration\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"Explicit Spark Job Test\") \\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"spark.driver.host\", \"jupyter\") \\\n",
    "        .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "        .config(\"spark.executor.instances\", \"3\") \\\n",
    "        .config(\"spark.executor.memory\", \"4g\") \\\n",
    "        .config(\"spark.executor.cores\", \"4\") \\\n",
    "        .config(\"spark.driver.memory\", \"2g\") \\\n",
    "        .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "        .config(\"spark.sql.caseSensitive\", \"false\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    print(f\"Spark version: {spark.version}\")\n",
    "    print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n",
    "    \n",
    "    return spark\n",
    "\n",
    "spark = init_spark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884d49bc-0523-4b10-8e86-04ac960270fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Defined.\n",
      "Successfully started reading from: hdfs:///data/raw/amazon_reviews/Kindle_Store.jsonl\n",
      "root\n",
      " |-- rating: float (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      " |-- helpful_vote: integer (nullable = true)\n",
      "\n",
      "Raw data count: 25577616\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, FloatType,\n",
    "                               ArrayType, LongType, BooleanType, IntegerType,\n",
    "                               TimestampType)\n",
    "from pyspark.sql.functions import from_unixtime, year, month, col\n",
    "\n",
    "HDFS_RAW_FILE = \"hdfs:///data/raw/amazon_reviews/Kindle_Store.jsonl\"\n",
    "HDFS_PROCESSED_DIR = \"hdfs:///data/processed/amazon_reviews/kindle_store\" # Thư mục chứa Parquet partitions\n",
    "HIVE_DATABASE_NAME = \"amazon_data\" # Tên database Hive (tạo nếu chưa có)\n",
    "HIVE_TABLE_NAME = \"kindle_reviews_processed\"\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Define Schema based on the image ---\n",
    "schema = StructType([\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"images\", ArrayType(StringType()), True), \n",
    "    StructField(\"asin\", StringType(), True),\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"timestamp\", LongType(), True),\n",
    "    StructField(\"verified_purchase\", BooleanType(), True),\n",
    "    StructField(\"helpful_vote\", IntegerType(), True)\n",
    "])\n",
    "print(\"Schema Defined.\")\n",
    "\n",
    "try:\n",
    "    df_raw = spark.read.schema(schema).json(HDFS_RAW_FILE)\n",
    "    print(f\"Successfully started reading from: {HDFS_RAW_FILE}\")\n",
    "    df_raw.printSchema() \n",
    "    df_raw.cache()\n",
    "    print(f\"Raw data count: {df_raw.count()}\") \n",
    "except Exception as e:\n",
    "    print(f\"Error reading raw data from {HDFS_RAW_FILE}: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18a0d01-8e62-4fec-9a23-257df2c078b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4623b90-f24b-4b83-a832-db024b0b328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= CLEANING ============\n",
      "\n",
      "+-------------+-------------------+----+-----+----------+\n",
      "|timestamp    |review_time        |year|month|date_str  |\n",
      "+-------------+-------------------+----+-----+----------+\n",
      "|1427541413000|2015-03-28 11:16:53|2015|3    |2015-03-28|\n",
      "|1504226946142|2017-09-01 00:49:06|2017|9    |2017-09-01|\n",
      "|1644883955777|2022-02-15 00:12:35|2022|2    |2022-02-15|\n",
      "|1363027885000|2013-03-11 18:51:25|2013|3    |2013-03-11|\n",
      "|1637557512064|2021-11-22 05:05:12|2021|11   |2021-11-22|\n",
      "|1637134078567|2021-11-17 07:27:58|2021|11   |2021-11-17|\n",
      "|1632291278732|2021-09-22 06:14:38|2021|9    |2021-09-22|\n",
      "|1614145710980|2021-02-24 05:48:30|2021|2    |2021-02-24|\n",
      "|1599452688091|2020-09-07 04:24:48|2020|9    |2020-09-07|\n",
      "|1574812541555|2019-11-26 23:55:41|2019|11   |2019-11-26|\n",
      "|1568214752013|2019-09-11 15:12:32|2019|9    |2019-09-11|\n",
      "|1567293346345|2019-08-31 23:15:46|2019|8    |2019-08-31|\n",
      "|1566774264228|2019-08-25 23:04:24|2019|8    |2019-08-25|\n",
      "|1558889036351|2019-05-26 16:43:56|2019|5    |2019-05-26|\n",
      "|1558333337162|2019-05-20 06:22:17|2019|5    |2019-05-20|\n",
      "|1546122375646|2018-12-29 22:26:15|2018|12   |2018-12-29|\n",
      "|1543044737414|2018-11-24 07:32:17|2018|11   |2018-11-24|\n",
      "|1542164901708|2018-11-14 03:08:21|2018|11   |2018-11-14|\n",
      "|1537226264482|2018-09-17 23:17:44|2018|9    |2018-09-17|\n",
      "|1537226257534|2018-09-17 23:17:37|2018|9    |2018-09-17|\n",
      "+-------------+-------------------+----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime, year, month, dayofmonth, date_format, coalesce, lit, trim, when, array\n",
    "\n",
    "print(\"============= CLEANING ============\\n\")\n",
    "df_cleaned = df_raw \\\n",
    "    .filter(col(\"text\").isNotNull() & (trim(col(\"text\")) != \"\")) \\\n",
    "    .filter(col(\"rating\").isNotNull()) \\\n",
    "    .filter(col(\"asin\").isNotNull() & (trim(col(\"asin\")) != \"\")) \\\n",
    "    .filter(col(\"user_id\").isNotNull() & (trim(col(\"user_id\")) != \"\")) \\\n",
    "    .withColumn(\"title\", trim(coalesce(col(\"title\"), lit(\"[no title]\")))) \\\n",
    "    .withColumn(\"text\", trim(col(\"text\"))) \\\n",
    "    .withColumn(\"asin\", trim(col(\"asin\"))) \\\n",
    "    .withColumn(\"parent_asin\", trim(col(\"parent_asin\"))) \\\n",
    "    .withColumn(\"user_id\", trim(col(\"user_id\"))) \\\n",
    "    .withColumn(\"helpful_vote\", coalesce(col(\"helpful_vote\"), lit(0)).cast(IntegerType())) \\\n",
    "    .withColumn(\"images\", coalesce(col(\"images\"), array().cast(ArrayType(StringType())))) \\\n",
    "    .withColumn(\"review_time\", from_unixtime(col(\"timestamp\") / 1000).cast(TimestampType())) \\\n",
    "    .filter(col(\"review_time\").isNotNull()) \n",
    "\n",
    "df_cleaned = df_cleaned.withColumn(\"year\", year(col(\"review_time\"))) \\\n",
    "                     .withColumn(\"month\", month(col(\"review_time\"))) \\\n",
    "                     .withColumn(\"day\", dayofmonth(col(\"review_time\")))\\\n",
    "                     .withColumn(\"date_str\", date_format(col(\"review_time\"), \"yyyy-MM-dd\")) \\\n",
    "                     .filter(col(\"year\").isNotNull() & (col(\"year\") >= 1990) & (col(\"year\") <= 2025))\n",
    "\n",
    "df_cleaned.select(\"timestamp\", \"review_time\", \"year\", \"month\", \"date_str\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dadb40-fa40-4de1-a673-7764775ebaa9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'deprecated' from 'typing_extensions' (/opt/conda/lib/python3.10/site-packages/typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgx\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataContextError\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofile\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser_configurable_profiler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserConfigurableProfiler\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _get_versions\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# great_expectations.data_context must be imported first or we will have circular dependency issues\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m  \u001b[38;5;66;03m# isort:skip\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_context\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/data_context/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     AbstractDataContext,\n\u001b[1;32m      3\u001b[0m     CloudDataContext,\n\u001b[1;32m      4\u001b[0m     EphemeralDataContext,\n\u001b[1;32m      5\u001b[0m     FileDataContext,\n\u001b[1;32m      6\u001b[0m     get_context,\n\u001b[1;32m      7\u001b[0m     project_manager,\n\u001b[1;32m      8\u001b[0m     set_context,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/data_context/data_context/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract_data_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     AbstractDataContext,\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud_data_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     CloudDataContext,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     get_context,\n\u001b[1;32m      9\u001b[0m     project_manager,\n\u001b[1;32m     10\u001b[0m     set_context,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/data_context/data_context/abstract_data_context.py:38\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgx_exceptions\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docs_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     deprecated_method_or_class,\n\u001b[1;32m     34\u001b[0m     new_argument,\n\u001b[1;32m     35\u001b[0m     new_method_or_class,\n\u001b[1;32m     36\u001b[0m     public_api,\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init \u001b[38;5;28;01mas\u001b[39;00m init_analytics\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m submit \u001b[38;5;28;01mas\u001b[39;00m submit_event\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ENV_CONFIG\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/analytics/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init, submit\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/analytics/client.py:9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UUID\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mposthog\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ENV_CONFIG, Config, update_config\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_event\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Event\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/analytics/config.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, Optional\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UUID\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompatibility\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BaseSettings,\n\u001b[1;32m      8\u001b[0m     GenericModel,\n\u001b[1;32m      9\u001b[0m     HttpUrl,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_EnvConfig\u001b[39;00m(BaseSettings):\n\u001b[1;32m     14\u001b[0m     gx_analytics_enabled: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/compatibility/pydantic.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompatibility\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnot_imported\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     is_version_greater_or_equal,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_version_greater_or_equal(version\u001b[38;5;241m=\u001b[39mpydantic\u001b[38;5;241m.\u001b[39mVERSION, compare_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# TODO: don't use star imports\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m import_module\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_migration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getattr_migration\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VERSION\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typing\u001b[38;5;241m.\u001b[39mTYPE_CHECKING:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# import of virtually everything is supported via `__getattr__` below,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# but we need them here for type checking and IDE support\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic/_migration.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version_short\n\u001b[1;32m      6\u001b[0m MOVED_IN_V2 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.utils:version_info\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.version:version_info\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.error_wrappers:ValidationError\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic:ValidationError\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.generics:GenericModel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.BaseModel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m DEPRECATED_MOVED_IN_V2 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.tools:schema_of\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.deprecated.tools:schema_of\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.tools:parse_obj_as\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.deprecated.tools:parse_obj_as\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.config:Extra\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.deprecated.config:Extra\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic/version.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The `version` module holds the version information for Pydantic.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations \u001b[38;5;28;01mas\u001b[39;00m _annotations\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m __pydantic_core_version__\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVERSION\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion_info\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m VERSION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.11.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic_core/__init__.py:30\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any \u001b[38;5;28;01mas\u001b[39;00m _Any\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pydantic_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     ArgsKwargs,\n\u001b[1;32m      8\u001b[0m     MultiHostUrl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     validate_core_schema,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoreConfig, CoreSchema, CoreSchemaType, ErrorType\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotRequired \u001b[38;5;28;01mas\u001b[39;00m _NotRequired\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic_core/core_schema.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pattern\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Literal, Union\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypedDict\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'deprecated' from 'typing_extensions' (/opt/conda/lib/python3.10/site-packages/typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "\n",
    "print(\"=========  VALIDATION ==========\\n\")\n",
    "\n",
    "df_validated = df_cleaned.withColumn(\"validation_error\", lit(None).cast(StringType()))\n",
    "\n",
    "# Áp dụng các quy tắc validation\n",
    "df_validated = df_validated.withColumn(\"validation_error\",\n",
    "    when((col(\"rating\") < 1.0) | (col(\"rating\") > 5.0), \"Invalid Rating\")\n",
    "    .otherwise(col(\"validation_error\"))) \n",
    "\n",
    "df_validated = df_validated.withColumn(\"validation_error\",\n",
    "    when(col(\"helpful_vote\") < 0, \"Negative Helpful Vote\")\n",
    "    .otherwise(col(\"validation_error\")))\n",
    "\n",
    "\n",
    "df_validated = df_validated.withColumn(\"validation_error\",\n",
    "    when(length(col(\"asin\")) != 10, \"Invalid ASIN Length\")\n",
    "    .otherwise(col(\"validation_error\")))\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "df_validated = df_validated.withColumn(\"validation_error\",\n",
    "    when(col(\"review_time\") > current_timestamp(), \"Future Timestamp\")\n",
    "    .otherwise(col(\"validation_error\")))\n",
    "\n",
    "\n",
    "df_valid = df_validated.filter(col(\"validation_error\").isNull()).drop(\"validation_error\")\n",
    "df_invalid = df_validated.filter(col(\"validation_error\").isNotNull())\n",
    "\n",
    "\n",
    "# Bảng Silver phụ\n",
    "HDFS_VALIDATION_FAILURES_DIR = \"hdfs:///data/processed/amazon_reviews/kindle_store_validation_failures\"\n",
    "print(f\"Writing validation failures to: {HDFS_VALIDATION_FAILURES_DIR}\")\n",
    "df_invalid.write \\\n",
    "    .partitionBy(\"year\", \"month\", \"validation_error\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(HDFS_VALIDATION_FAILURES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec5bb2b-47c6-45eb-8ba8-34f8c6877e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========  VALIDATION ==========\n",
      "\n",
      "Writing validation failures to: hdfs:///data/processed/amazon_reviews/kindle_store_validation_failures\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "print(\"============== STANRDALIZATION ===========\\n\")\n",
    "df_standardized = df_valid \\\n",
    "    .withColumn(\"title_processed\", lower(col(\"title\"))) \\\n",
    "    .withColumn(\"text_processed\", lower(col(\"text\")))\n",
    "    \n",
    "# df_standardized.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad723ec-3073-47f2-81f4-6c811522eeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== STANRDALIZATION ===========\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25576202"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\", \"asin\").orderBy(col(\"review_time\").desc())\n",
    "\n",
    "df_deduplicated = df_standardized.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "                               .filter(col(\"rn\") == 1) \\\n",
    "                               .drop(\"rn\")\n",
    "    \n",
    "# print(f\"Deduplication complete. Count after deduplication: {df_deduplicated.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce92ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_processed = df_deduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb765671-0894-4f13-8f6c-2c929e69e74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication complete. Count after deduplication: 25300905\n"
     ]
    }
   ],
   "source": [
    "HDFS_PROCESSED_BASE = \"hdfs:///data/processed/amazon_reviews\"\n",
    "\n",
    "# Hive Database Names\n",
    "HIVE_PROCESSED_DB = \"processed\"\n",
    "\n",
    "HDFS_PROCESSED_MAIN_DIR = f\"{HDFS_PROCESSED_BASE}/kindle_store_main\"\n",
    "HIVE_PROCESSED_MAIN_TABLE = \"kindle_reviews_main\"\n",
    "\n",
    "# Validation Failures Table \n",
    "HDFS_VALIDATION_FAILURES_DIR = f\"{HDFS_PROCESSED_BASE}/kindle_store_validation_failures\"\n",
    "HIVE_VALIDATION_FAILURES_TABLE = \"kindle_reviews_validation_failures\"\n",
    "\n",
    "# Product Dimension VIEW \n",
    "HIVE_PRODUCTS_DIM_VIEW = \"kindle_products_dim\"\n",
    "\n",
    "# User Dimension VIEW \n",
    "HIVE_USERS_DIM_VIEW = \"kindle_users_dim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec98202-8c82-4194-bf90-83239693c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = df_final_processed.select(\n",
    "    \"rating\", \"title_processed\", \"text_processed\", \"images\", \"asin\",\n",
    "    \"parent_asin\", \"user_id\", \"verified_purchase\", \"helpful_vote\",\n",
    "    \"review_time\", \"year\", \"month\", \"day\", \"date_str\"\n",
    ")\n",
    "\n",
    "print(f\"Writing main processed data to: {HDFS_PROCESSED_MAIN_DIR}\")\n",
    "df_to_save.write \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(HDFS_PROCESSED_MAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4885f01-0c5e-470d-a0b2-207713eac9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Creating/Updating Hive Tables and VIEWS ---\")\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {HIVE_PROCESSED_DB}\")\n",
    "print(f\"Ensured Hive database '{HIVE_PROCESSED_DB}' exists.\")\n",
    "\n",
    "# Create Hive External TABLE for Main Processed Data\n",
    "print(f\"Creating Hive TABLE: {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE}\")\n",
    "create_main_table_sql = f\"\"\"\n",
    "CREATE EXTERNAL TABLE {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE} (\n",
    "    rating FLOAT,\n",
    "    title_processed STRING,\n",
    "    text_processed STRING,\n",
    "    images ARRAY<STRING>,\n",
    "    asin STRING,\n",
    "    parent_asin STRING,\n",
    "    user_id STRING,\n",
    "    verified_purchase BOOLEAN,\n",
    "    helpful_vote INT,\n",
    "    review_time TIMESTAMP,\n",
    "    date_str STRING\n",
    ")\n",
    "PARTITIONED BY (year INT, month INT)\n",
    "STORED AS PARQUET\n",
    "LOCATION '{HDFS_PROCESSED_MAIN_DIR}'\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "try:\n",
    "    spark.sql(create_main_table_sql)\n",
    "    print(f\"Running MSCK REPAIR TABLE for {HIVE_PROCESSED_MAIN_TABLE}...\")\n",
    "    spark.sql(f\"MSCK REPAIR TABLE {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE}\")\n",
    "    print(f\"Successfully created and repaired main processed table.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR creating/repairing main processed table: {e}\")\n",
    "\n",
    "\n",
    "# Create Hive External TABLE for Validation Failures (vẫn giữ nguyên)\n",
    "print(f\"Creating Hive TABLE: {HIVE_PROCESSED_DB}.{HIVE_VALIDATION_FAILURES_TABLE}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_PROCESSED_DB}.{HIVE_VALIDATION_FAILURES_TABLE}\")\n",
    "create_failures_table_sql = f\"\"\"\n",
    "CREATE EXTERNAL TABLE {HIVE_PROCESSED_DB}.{HIVE_VALIDATION_FAILURES_TABLE} (\n",
    "    rating FLOAT,\n",
    "    title STRING,\n",
    "    text STRING,\n",
    "    asin STRING,\n",
    "    parent_asin STRING,\n",
    "    user_id STRING,\n",
    "    verified_purchase BOOLEAN,\n",
    "    helpful_vote INT,\n",
    "    timestamp LONG,\n",
    "    review_time TIMESTAMP,\n",
    "    date_str STRING\n",
    ")\n",
    "PARTITIONED BY (year INT, month INT, validation_error STRING)\n",
    "STORED AS PARQUET\n",
    "LOCATION '{HDFS_VALIDATION_FAILURES_DIR}'\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "try:\n",
    "    spark.sql(create_failures_table_sql)\n",
    "    print(f\"Running MSCK REPAIR TABLE for {HIVE_VALIDATION_FAILURES_TABLE}...\")\n",
    "    spark.sql(f\"MSCK REPAIR TABLE {HIVE_PROCESSED_DB}.{HIVE_VALIDATION_FAILURES_TABLE}\")\n",
    "    print(f\"Successfully created and repaired validation failures table.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR creating/repairing validation failures table: {e}\")\n",
    "\n",
    "\n",
    "# Create Hive VIEW for Products Dimension\n",
    "print(f\"Creating Hive VIEW: {HIVE_PROCESSED_DB}.{HIVE_PRODUCTS_DIM_VIEW}\")\n",
    "spark.sql(f\"DROP VIEW IF EXISTS {HIVE_PROCESSED_DB}.{HIVE_PRODUCTS_DIM_VIEW}\")\n",
    "create_products_view_sql = f\"\"\"\n",
    "CREATE VIEW {HIVE_PROCESSED_DB}.{HIVE_PRODUCTS_DIM_VIEW} AS\n",
    "SELECT\n",
    "    asin,\n",
    "    parent_asin,\n",
    "    MIN(review_time) AS first_review_time,\n",
    "    MAX(review_time) AS last_review_time,\n",
    "    AVG(rating) AS avg_rating,\n",
    "    COUNT(*) AS total_reviews,\n",
    "    SUM(helpful_vote) AS total_helpful_votes_received\n",
    "FROM {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE}\n",
    "GROUP BY asin, parent_asin\n",
    "\"\"\"\n",
    "try:\n",
    "    spark.sql(create_products_view_sql)\n",
    "    print(f\"Successfully created products dimension VIEW.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR creating products dimension VIEW: {e}\")\n",
    "\n",
    "\n",
    "# Create Hive VIEW for Users Dimension\n",
    "print(f\"Creating Hive VIEW: {HIVE_PROCESSED_DB}.{HIVE_USERS_DIM_VIEW}\")\n",
    "spark.sql(f\"DROP VIEW IF EXISTS {HIVE_PROCESSED_DB}.{HIVE_USERS_DIM_VIEW}\")\n",
    "create_users_view_sql = f\"\"\"\n",
    "CREATE VIEW {HIVE_PROCESSED_DB}.{HIVE_USERS_DIM_VIEW} AS\n",
    "SELECT\n",
    "    user_id,\n",
    "    MIN(review_time) AS first_review_time,\n",
    "    MAX(review_time) AS last_review_time,\n",
    "    COUNT(*) AS total_reviews_written,\n",
    "    AVG(rating) AS avg_rating_given,\n",
    "    SUM(helpful_vote) AS total_helpful_votes_on_written_reviews\n",
    "FROM {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE}\n",
    "GROUP BY user_id\n",
    "\"\"\"\n",
    "try:\n",
    "    spark.sql(create_users_view_sql)\n",
    "    print(f\"Successfully created users dimension VIEW.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR creating users dimension VIEW: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Finished Simplified Storage and Hive Table/View Creation (Phase 3) ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c571a224-0f1a-4c64-be17-f95de433f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main processed data to: hdfs:///data/processed/amazon_reviews/kindle_store_main\n"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68933c6",
   "metadata": {},
   "source": [
    "## ENRICH AND JOIN WITH METADATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88751490-0142-4239-9557-bda46da2e339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating/Updating Hive Tables and VIEWS ---\n",
      "Ensured Hive database 'processed' exists.\n",
      "Creating Hive TABLE: processed.kindle_reviews_main\n",
      "Running MSCK REPAIR TABLE for kindle_reviews_main...\n",
      "Successfully created and repaired main processed table.\n",
      "Creating Hive TABLE: processed.kindle_reviews_validation_failures\n",
      "Running MSCK REPAIR TABLE for kindle_reviews_validation_failures...\n",
      "Successfully created and repaired validation failures table.\n",
      "Creating Hive VIEW: processed.kindle_products_dim\n",
      "Successfully created products dimension VIEW.\n",
      "Creating Hive VIEW: processed.kindle_users_dim\n",
      "Successfully created users dimension VIEW.\n",
      "\n",
      "--- Finished Simplified Storage and Hive Table/View Creation (Phase 3) ---\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (StringType, ArrayType, StructType,\n",
    "                               StructField, FloatType, IntegerType, LongType,\n",
    "                               BooleanType, TimestampType, MapType)\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "\n",
    "HIVE_PROCESSED_DB = \"processed\"\n",
    "HIVE_CURATED_DB = \"curated\"\n",
    "\n",
    "# Input Tables/Files\n",
    "HIVE_PROCESSED_TABLE = \"kindle_reviews_main\"\n",
    "HDFS_RAW_METADATA_PATH = \"hdfs:///data/raw/amazon_reviews/meta_Kindle_Store.jsonl\"\n",
    "\n",
    "\n",
    "# Base Table\n",
    "HDFS_CURATED_BASE_DIR = \"hdfs:///data/curated/amazon_reviews/kindle_store_base\"\n",
    "HIVE_CURATED_BASE_TABLE = \"kindle_reviews_base\"\n",
    "\n",
    "# Materialized Summary Tables (MỚI)\n",
    "HDFS_PRODUCT_MONTHLY_SUMMARY_DIR = \"hdfs:///data/curated/amazon_reviews/product_monthly_summary_materialized\"\n",
    "HIVE_PRODUCT_MONTHLY_SUMMARY_TABLE = \"product_monthly_summary_materialized\"\n",
    "\n",
    "HDFS_PARENT_PRODUCT_OVERALL_SUMMARY_DIR = \"hdfs:///data/curated/amazon_reviews/parent_product_overall_summary_materialized\"\n",
    "HIVE_PARENT_PRODUCT_OVERALL_TABLE = \"parent_product_overall_summary_materialized\"\n",
    "\n",
    "HDFS_PRODUCT_OVERALL_SUMMARY_DIR = \"hdfs:///data/curated/amazon_reviews/product_overall_summary_materialized\" \n",
    "HIVE_PRODUCT_OVERALL_SUMMARY_TABLE = \"product_overall_summary_materialized\" \n",
    "\n",
    "HDFS_REVIEW_STATS_MONTHLY_DIR = \"hdfs:///data/curated/amazon_reviews/review_stats_monthly_materialized\" \n",
    "HIVE_REVIEW_STATS_MONTHLY_TABLE = \"review_stats_monthly_materialized\" \n",
    "\n",
    "HIVE_MONTHLY_STATS_VIEW = \"review_stats_monthly\" \n",
    "\n",
    "# VIEWs (Sẽ bị xóa và thay bằng TABLE)\n",
    "HIVE_PRODUCT_MONTHLY_VIEW_OLD = \"product_monthly_summary\"\n",
    "HIVE_PARENT_PRODUCT_OVERALL_VIEW_OLD = \"parent_product_overall_summary\"\n",
    "HIVE_PRODUCT_OVERALL_VIEW_OLD = \"product_overall_summary\"\n",
    "HIVE_MONTHLY_STATS_VIEW_OLD = \"review_stats_monthly\" \n",
    "\n",
    "# Default Values\n",
    "DEFAULT_TITLE = \"[Unknown Parent Title]\"\n",
    "DEFAULT_MAIN_CATEGORY = \"[Unknown Main Category]\"\n",
    "DEFAULT_CATEGORY = \"[Unknown Parent Category]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d192657-9d3c-433a-91b8-528fece3e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Định nghĩa schema của bảng metadata\n",
    "\n",
    "metadata_schema = StructType([\n",
    "    StructField(\"main_category\", StringType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"average_rating\", FloatType(), True),\n",
    "    StructField(\"rating_number\", IntegerType(), True),\n",
    "    StructField(\"features\", ArrayType(StringType()), True),\n",
    "    StructField(\"description\", ArrayType(StringType()), True),\n",
    "    StructField(\"price\", FloatType(), True),\n",
    "    StructField(\"images\", ArrayType(MapType(StringType(), StringType())), True),\n",
    "    StructField(\"videos\", ArrayType(MapType(StringType(), StringType())), True),\n",
    "    StructField(\"store\", StringType(), True),\n",
    "    StructField(\"categories\", ArrayType(StringType()), True),\n",
    "    StructField(\"details\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"parent_asin\", StringType(), True),\n",
    "    StructField(\"bought_together\", ArrayType(StringType()), True)\n",
    "])\n",
    "\n",
    "def process_metadata(spark: SparkSession) -> 'pyspark.sql.DataFrame':\n",
    "    \"\"\"Đọc, xử lý và cache dữ liệu metadata.\"\"\"\n",
    "    print(f\"--- [Metadata Processing] Starting ---\")\n",
    "    print(f\"Reading metadata from: {HDFS_RAW_METADATA_PATH}\")\n",
    "    start_meta = time.time()\n",
    "    try:\n",
    "\n",
    "        df_meta_raw = spark.read.schema(metadata_schema).json(HDFS_RAW_METADATA_PATH)\n",
    "        # print(f\"Read {df_meta_raw.count()} raw metadata records.\")\n",
    "\n",
    "        df_meta_filtered = df_meta_raw.filter(F.col(\"parent_asin\").isNotNull() & (F.trim(F.col(\"parent_asin\")) != \"\"))\n",
    "\n",
    "        # Lấy phần tử cuối của Array<String> ***\n",
    "        df_meta_processed = df_meta_filtered.withColumn(\n",
    "            \"parent_category_extracted\",\n",
    "            F.when(F.size(F.col(\"categories\")) > 0, \n",
    "                   F.element_at(F.col(\"categories\"), -1)) \n",
    "             .otherwise(None)\n",
    "        )\n",
    "\n",
    "        # Chọn final columns, clean, handle nulls, drop duplicates\n",
    "        df_metadata = df_meta_processed.select(\n",
    "                F.trim(F.col(\"parent_asin\")).alias(\"parent_asin\"),\n",
    "                F.trim(F.coalesce(F.col(\"title\"), F.lit(DEFAULT_TITLE))).alias(\"parent_title\"),\n",
    "                F.trim(F.coalesce(F.col(\"main_category\"), F.lit(DEFAULT_MAIN_CATEGORY))).alias(\"main_category\"),\n",
    "                F.trim(F.coalesce(F.col(\"parent_category_extracted\"), F.lit(DEFAULT_CATEGORY))).alias(\"parent_category\")\n",
    "            ).dropDuplicates([\"parent_asin\"])\n",
    "\n",
    "        # Cache the processed metadata\n",
    "        df_metadata.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "        meta_count = df_metadata.count() # Trigger cache population\n",
    "        print(f\"Finished processing metadata. Count after deduplication: {meta_count}\")\n",
    "        print(f\"Metadata processing took {time.time() - start_meta:.2f} seconds.\")\n",
    "        print(f\"--- [Metadata Processing] Finished ---\")\n",
    "        return df_metadata\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"!!! ERROR during Metadata Processing: {e}\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        traceback.print_exc()\n",
    "        raise e "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d7e89-746f-4b28-90de-40363a6eda9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_spark()\n",
    "spark = None\n",
    "start_time_job = time.time()\n",
    "print(\"==========================================================\")\n",
    "print(\"=== Create Base Curated & Views (with Metadata) ===\")\n",
    "print(\"==========================================================\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create Base Curated Data (with Metadata) and Views\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.driver.host\", \"jupyter\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Session created. UI: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# df_metadata = process_metadata(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d8641-7523-4305-a824-3dbe0e31fe9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Read Silver Table  (Reviews)\n",
    "print(f\"\\n--- [Reviews Reading] Starting ---\")\n",
    "print(f\"Reading data from Silver table: {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_TABLE}\")\n",
    "start_read_silver = time.time()\n",
    "df_silver = spark.sql(f\"SELECT * FROM {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_TABLE}\")\n",
    "\n",
    "print(f\"Finished reading Silver data in {time.time() - start_read_silver:.2f} seconds.\")\n",
    "print(f\"--- [Reviews Reading] Finished ---\")\n",
    "\n",
    "# 3. Join Silver data with Metadata (Join bằng parent_asin)\n",
    "print(f\"\\n--- [Joining Reviews and Metadata] Starting ---\")\n",
    "start_join = time.time()\n",
    "df_joined = df_silver.join(\n",
    "\n",
    "    F.broadcast(df_metadata),\n",
    "    on=\"parent_asin\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(f\"Finished joining data in {time.time() - start_join:.2f} seconds.\")\n",
    "# print(f\"Joined DataFrame count: {joined_count}\")\n",
    "print(f\"--- [Joining Reviews and Metadata] Finished ---\")\n",
    "\n",
    "\n",
    "# 4. Chọn lọc cột cuối cùng cho bảng Base Curated\n",
    "print(f\"\\n--- [Selecting Final Columns] Starting ---\")\n",
    "\n",
    "df_base_curated = df_joined.select(\n",
    "    F.col(\"parent_asin\"), F.col(\"asin\"), F.col(\"user_id\"), F.col(\"review_time\"),\n",
    "    F.col(\"year\"), F.col(\"month\"), F.col(\"date_str\"), F.col(\"rating\"), F.col(\"helpful_vote\"),\n",
    "    F.col(\"verified_purchase\"), F.col(\"text_processed\"),\n",
    "    F.coalesce(F.col(\"parent_title\"), F.lit(DEFAULT_TITLE)).alias(\"parent_title\"),\n",
    "    F.coalesce(F.col(\"main_category\"), F.lit(DEFAULT_MAIN_CATEGORY)).alias(\"main_category\"),\n",
    "    F.coalesce(F.col(\"parent_category\"), F.lit(DEFAULT_CATEGORY)).alias(\"parent_category\"),\n",
    "    F.col(\"images\")\n",
    ")\n",
    "\n",
    "\n",
    "df_base_curated.persist(StorageLevel.MEMORY_AND_DISK) \n",
    "base_count = df_base_curated.count() # Trigger join và cache\n",
    "print(f\"Base Curated DataFrame selected and cached. Approx Count: {base_count}\")\n",
    "print(f\"Schema for Base Curated Table:\")\n",
    "df_base_curated.printSchema()\n",
    "print(f\"--- [Selecting Final Columns for Base] Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001a851a-5cc9-4bff-8246-2f0ff9f5d4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Ghi bảng Base Curated vào HDFS (ACTION)\n",
    "print(f\"\\n--- [Writing Base Curated Table] Starting ---\")\n",
    "print(f\"Writing Base Curated data (with metadata) to: {HDFS_CURATED_BASE_DIR}\")\n",
    "start_write = time.time()\n",
    "(df_base_curated.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(HDFS_CURATED_BASE_DIR)\n",
    ")\n",
    "print(f\"Successfully wrote Base Curated Parquet data in {time.time() - start_write:.2f} seconds.\")\n",
    "print(f\"--- [Writing Base Curated Table] Finished ---\")\n",
    "\n",
    "# 6. Tạo/Cập nhật bảng Hive External cho Base Curated Data\n",
    "print(f\"\\n--- [Creating/Repairing Hive Base Table] Starting ---\")\n",
    "start_hive_base = time.time()\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {HIVE_CURATED_DB}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_CURATED_DB}.{HIVE_CURATED_BASE_TABLE}\")\n",
    "create_base_table_sql_updated = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {HIVE_CURATED_DB}.{HIVE_CURATED_BASE_TABLE} (\n",
    "        parent_asin STRING, asin STRING, user_id STRING, review_time TIMESTAMP,\n",
    "        date_str STRING, rating FLOAT, helpful_vote INT, verified_purchase BOOLEAN,\n",
    "        text_processed STRING, parent_title STRING, main_category STRING,\n",
    "        parent_category STRING, images ARRAY<STRING>\n",
    "    ) PARTITIONED BY (year INT, month INT) STORED AS PARQUET\n",
    "    LOCATION '{HDFS_CURATED_BASE_DIR}' \n",
    "    TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_base_table_sql_updated)\n",
    "spark.sql(f\"MSCK REPAIR TABLE {HIVE_CURATED_DB}.{HIVE_CURATED_BASE_TABLE}\")\n",
    "print(f\"Successfully created/repaired Base Curated table in {time.time() - start_hive_base:.2f} seconds.\")\n",
    "print(f\"--- [Creating/Repairing Hive Base Table] Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403911e7-8df9-4ca3-800d-ada37b7635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Tính toán Product Monthly Summary\n",
    "print(f\"\\n--- [Calculating Product Monthly Summary] Starting ---\")\n",
    "start_calc_monthly = time.time()\n",
    "df_product_monthly_summary = df_base_curated.groupBy(\"year\", \"month\", \"parent_asin\", \"asin\").agg(\n",
    "    F.count(\"*\").alias(\"total_reviews\"),\n",
    "    F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "    F.sum(\"helpful_vote\").alias(\"total_helpful_votes\"),\n",
    "    F.sum(F.when(F.col(\"verified_purchase\") == True, 1).otherwise(0)).alias(\"total_verified_reviews\")\n",
    ")\n",
    "print(f\"Calculated Product Monthly Summary in {time.time() - start_calc_monthly:.2f} seconds.\")\n",
    "print(f\"--- [Calculating Product Monthly Summary] Finished ---\")\n",
    "        \n",
    "# 8. Ghi Product Monthly Summary vào HDFS\n",
    "print(f\"\\n--- [Writing Product Monthly Summary Table] Starting ---\")\n",
    "start_write_monthly = time.time()\n",
    "(df_product_monthly_summary.write\n",
    "    .partitionBy(\"year\") \n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(HDFS_PRODUCT_MONTHLY_SUMMARY_DIR)\n",
    ")\n",
    "print(f\"Successfully wrote Product Monthly Summary Parquet data in {time.time() - start_write_monthly:.2f} seconds.\")\n",
    "print(f\"--- [Writing Product Monthly Summary Table] Finished ---\")\n",
    "\n",
    "# 9. Tạo Hive Table cho Product Monthly Summary\n",
    "print(f\"\\n--- [Creating Hive Product Monthly Summary Table (Partitioned by Year)] Starting ---\")\n",
    "start_hive_monthly = time.time()\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_CURATED_DB}.{HIVE_PRODUCT_MONTHLY_SUMMARY_TABLE}\")\n",
    "spark.sql(f\"DROP VIEW IF EXISTS {HIVE_CURATED_DB}.{HIVE_PRODUCT_MONTHLY_VIEW_OLD}\")\n",
    "\n",
    "create_monthly_summary_table_sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {HIVE_CURATED_DB}.{HIVE_PRODUCT_MONTHLY_SUMMARY_TABLE} (\n",
    "        month INT, parent_asin STRING, asin STRING, total_reviews BIGINT,\n",
    "        avg_rating DOUBLE, total_helpful_votes BIGINT, total_verified_reviews BIGINT\n",
    "    )\n",
    "    PARTITIONED BY (year INT)\n",
    "    STORED AS PARQUET LOCATION '{HDFS_PRODUCT_MONTHLY_SUMMARY_DIR}'\n",
    "    TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "spark.sql(create_monthly_summary_table_sql)\n",
    "\n",
    "print(f\"Table created. Running MSCK REPAIR TABLE for {HIVE_PRODUCT_MONTHLY_SUMMARY_TABLE}...\")\n",
    "spark.sql(f\"MSCK REPAIR TABLE {HIVE_CURATED_DB}.{HIVE_PRODUCT_MONTHLY_SUMMARY_TABLE}\")\n",
    "print(f\"Successfully created/repaired Product Monthly Summary table in {time.time() - start_hive_monthly:.2f} seconds.\")\n",
    "print(f\"--- [Creating Hive Product Monthly Summary Table] Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79831a01-6fce-4434-9df1-1ebbfa561d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|         parent_asin|              string|   null|\n",
      "|       main_category|              string|   null|\n",
      "|           cat_group|                 int|   null|\n",
      "|            cat_path|              string|   null|\n",
      "|# Partition Infor...|                    |       |\n",
      "|          # col_name|           data_type|comment|\n",
      "|           cat_group|                 int|   null|\n",
      "|            cat_path|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             curated|       |\n",
      "|               Table|       item_metadata|       |\n",
      "|               Owner|              jovyan|       |\n",
      "|        Created Time|Wed Apr 02 05:42:...|       |\n",
      "|         Last Access|             UNKNOWN|       |\n",
      "|          Created By|         Spark 3.3.0|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|hdfs://namenode:9...|       |\n",
      "|       Serde Library|org.apache.hadoop...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------------+\n",
      "|total_records|\n",
      "+-------------+\n",
      "|      1591371|\n",
      "+-------------+\n",
      "\n",
      "+-----------+-------------+---------+--------------------+\n",
      "|parent_asin|main_category|cat_group|            cat_path|\n",
      "+-----------+-------------+---------+--------------------+\n",
      "| B07MJCDHBT| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B07CGLFP35| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B09H6F588Y| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B081DT5WN5| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B07FBDB2QP| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B00A023TYM| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B09VC8KG1V| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B07BC7R8PJ| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B005D8J2EY| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B0B2NC5TMM| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "+-----------+-------------+---------+--------------------+\n",
      "\n",
      "+-----------+-------------+---------+--------------------+\n",
      "|parent_asin|main_category|cat_group|            cat_path|\n",
      "+-----------+-------------+---------+--------------------+\n",
      "| B00FRL1UYM| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B07FFDWF8L| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B07HYXX3HX| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B01M1IKRMV| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B0084ASXQG| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B007B4Q2T6| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B00HXYM7DY| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B009KAAW2M| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B00DPM7VZW| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "| B008K88C0Y| Buy a Kindle|        0|Kindle Store > Ki...|\n",
      "+-----------+-------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 10. Tính toán Parent Product Overall Summary\n",
    "print(f\"\\n--- [Calculating Parent Product Overall Summary] Starting ---\")\n",
    "start_calc_parent = time.time()\n",
    "\n",
    "# Cách khác để tạo Map rating distribution (có thể hiệu quả hơn)\n",
    "df_parent_overall_summary = df_base_curated.groupBy(\"parent_asin\").agg(\n",
    "     F.first(\"parent_title\").alias(\"parent_title\"),\n",
    "     F.first(\"main_category\").alias(\"main_category\"),\n",
    "     F.first(\"parent_category\").alias(\"parent_category\"),\n",
    "     F.min(\"review_time\").alias(\"first_review_time\"),\n",
    "     F.max(\"review_time\").alias(\"last_review_time\"),\n",
    "     F.countDistinct(\"asin\").alias(\"num_variants\"),\n",
    "     F.count(\"*\").alias(\"total_reviews\"),\n",
    "     F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "     F.sum(\"helpful_vote\").alias(\"total_helpful_votes\"),\n",
    "     F.sum(F.when(F.col(\"verified_purchase\") == True, 1).otherwise(0)).alias(\"total_verified_reviews\"),\n",
    "     F.expr(\"\"\"\n",
    "         map(\n",
    "             1.0f, count(case when rating = 1.0 then 1 end),\n",
    "             2.0f, count(case when rating = 2.0 then 1 end),\n",
    "             3.0f, count(case when rating = 3.0 then 1 end),\n",
    "             4.0f, count(case when rating = 4.0 then 1 end),\n",
    "             5.0f, count(case when rating = 5.0 then 1 end)\n",
    "         )\n",
    "     \"\"\").alias(\"rating_distribution\")\n",
    ")\n",
    "print(f\"Calculated Parent Product Overall Summary in {time.time() - start_calc_parent:.2f} seconds.\")\n",
    "print(f\"--- [Calculating Parent Product Overall Summary] Finished ---\")\n",
    "\n",
    "\n",
    "# 11. Ghi Parent Product Overall Summary vào HDFS\n",
    "print(f\"\\n--- [Writing Parent Product Overall Summary Table] Starting ---\")\n",
    "start_write_parent = time.time()\n",
    "(df_parent_overall_summary.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(HDFS_PARENT_PRODUCT_OVERALL_SUMMARY_DIR)\n",
    ")\n",
    "print(f\"Successfully wrote Parent Product Overall Summary Parquet data in {time.time() - start_write_parent:.2f} seconds.\")\n",
    "print(f\"--- [Writing Parent Product Overall Summary Table] Finished ---\")\n",
    "\n",
    "# 12. Tạo Hive Table cho Parent Product Overall Summary\n",
    "print(f\"\\n--- [Creating Hive Parent Product Overall Summary Table] Starting ---\")\n",
    "start_hive_parent = time.time()\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_CURATED_DB}.{HIVE_PARENT_PRODUCT_OVERALL_TABLE}\")\n",
    "spark.sql(f\"DROP VIEW IF EXISTS {HIVE_CURATED_DB}.{HIVE_PARENT_PRODUCT_OVERALL_VIEW_OLD}\") \n",
    "\n",
    "create_parent_summary_table_sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {HIVE_CURATED_DB}.{HIVE_PARENT_PRODUCT_OVERALL_TABLE} (\n",
    "        parent_asin STRING, parent_title STRING, main_category STRING, parent_category STRING,\n",
    "        first_review_time TIMESTAMP, last_review_time TIMESTAMP, num_variants BIGINT,\n",
    "        total_reviews BIGINT, avg_rating DOUBLE, total_helpful_votes BIGINT,\n",
    "        total_verified_reviews BIGINT, rating_distribution MAP<FLOAT, BIGINT>\n",
    "    ) STORED AS PARQUET LOCATION '{HDFS_PARENT_PRODUCT_OVERALL_SUMMARY_DIR}'\n",
    "    TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "spark.sql(create_parent_summary_table_sql)\n",
    "print(f\"Successfully created Parent Product Overall Summary table in {time.time() - start_hive_parent:.2f} seconds.\")\n",
    "print(f\"--- [Creating Hive Parent Product Overall Summary Table] Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32b609a-f7aa-4b90-8f77-5c626efb942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+---------+--------------------+\n",
      "|parent_asin|main_category|cat_group|            cat_path|\n",
      "+-----------+-------------+---------+--------------------+\n",
      "| B004C6S25I| Buy a Kindle|       -2|Kindle Store > Ki...|\n",
      "| B00499R2TU| Buy a Kindle|       -2|Kindle Store > Ki...|\n",
      "+-----------+-------------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 13. Tính toán Product (ASIN) Overall Summary\n",
    "print(f\"\\n--- [Calculating Product (ASIN) Overall Summary] Starting ---\")\n",
    "start_calc_asin = time.time()\n",
    "df_product_overall_summary = df_base_curated.groupBy(\"parent_asin\", \"asin\").agg(\n",
    "     F.first(\"parent_title\", ignorenulls=True).alias(\"parent_title\"),\n",
    "     F.first(\"parent_category\", ignorenulls=True).alias(\"parent_category\"),\n",
    "     F.min(\"review_time\").alias(\"first_review_time\"),\n",
    "     F.max(\"review_time\").alias(\"last_review_time\"),\n",
    "     F.count(\"*\").alias(\"total_reviews\"),\n",
    "     F.avg(\"rating\").alias(\"avg_rating\"),\n",
    "     F.sum(\"helpful_vote\").alias(\"total_helpful_votes\"),\n",
    "     F.sum(F.when(F.col(\"verified_purchase\") == True, 1).otherwise(0)).alias(\"total_verified_reviews\"),\n",
    "     F.expr(\"map(1.0f, count(case when rating = 1.0 then 1 end), 2.0f, count(case when rating = 2.0 then 1 end), 3.0f, count(case when rating = 3.0 then 1 end), 4.0f, count(case when rating = 4.0 then 1 end), 5.0f, count(case when rating = 5.0 then 1 end))\").alias(\"rating_distribution\")\n",
    ").withColumn(\"first_letter_parent_asin\", F.substring(F.col(\"parent_asin\"), 1, 1)) \n",
    "print(f\"Calculated Product (ASIN) Overall Summary in {time.time() - start_calc_asin:.2f} seconds.\")\n",
    "print(f\"--- [Calculating Product (ASIN) Overall Summary] Finished ---\")\n",
    "\n",
    "# 13. Ghi Product (ASIN) Overall Summary vào HDFS\n",
    "print(f\"\\n--- [Writing Product (ASIN) Overall Summary Table (Partitioned by First Letter)] Starting ---\")\n",
    "start_write_asin = time.time()\n",
    "(df_product_overall_summary.write\n",
    "    .partitionBy(\"first_letter_parent_asin\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(HDFS_PRODUCT_OVERALL_SUMMARY_DIR)\n",
    ")\n",
    "print(f\"Successfully wrote Product (ASIN) Overall Summary Parquet data in {time.time() - start_write_asin:.2f} seconds.\")\n",
    "print(f\"--- [Writing Product (ASIN) Overall Summary Table] Finished ---\")\n",
    "\n",
    "\n",
    "# 14. Hive Table cho Product (ASIN) Overall Summary\n",
    "print(f\"\\n--- [Creating Hive Product (ASIN) Overall Summary Table (Partitioned by First Letter)] Starting ---\")\n",
    "start_hive_asin = time.time()\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_CURATED_DB}.{HIVE_PRODUCT_OVERALL_SUMMARY_TABLE}\")\n",
    "spark.sql(f\"DROP VIEW IF EXISTS {HIVE_CURATED_DB}.{HIVE_PRODUCT_OVERALL_VIEW_OLD}\")\n",
    "\n",
    "create_asin_summary_table_sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {HIVE_CURATED_DB}.{HIVE_PRODUCT_OVERALL_SUMMARY_TABLE} (\n",
    "        parent_asin STRING, asin STRING, parent_title STRING, parent_category STRING,\n",
    "        first_review_time TIMESTAMP, last_review_time TIMESTAMP,\n",
    "        total_reviews BIGINT, avg_rating DOUBLE, total_helpful_votes BIGINT,\n",
    "        total_verified_reviews BIGINT, rating_distribution MAP<FLOAT, BIGINT>\n",
    "    )\n",
    "    PARTITIONED BY (first_letter_parent_asin STRING) -- <<< PARTITIONED BY FIRST LETTER\n",
    "    STORED AS PARQUET LOCATION '{HDFS_PRODUCT_OVERALL_SUMMARY_DIR}'\n",
    "    TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "spark.sql(create_asin_summary_table_sql)\n",
    "\n",
    "\n",
    "print(f\"Table created. Running MSCK REPAIR TABLE for {HIVE_PRODUCT_OVERALL_SUMMARY_TABLE}...\")\n",
    "spark.sql(f\"MSCK REPAIR TABLE {HIVE_CURATED_DB}.{HIVE_PRODUCT_OVERALL_SUMMARY_TABLE}\")\n",
    "print(f\"Successfully created/repaired Product (ASIN) Overall Summary table in {time.time() - start_hive_asin:.2f} seconds.\")\n",
    "print(f\"--- [Creating Hive Product (ASIN) Overall Summary Table] Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7487a4-16f9-43dd-bc96-e0ddc38e4c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+--------+\n",
      "|parent_asin|main_category|cat_path|\n",
      "+-----------+-------------+--------+\n",
      "+-----------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#         Stats Monthly\n",
    "print(f\"\\n--- [Calculating Review Stats Monthly] Starting ---\")\n",
    "start_calc_stats = time.time()\n",
    "df_review_stats_monthly = df_product_monthly_summary.groupBy(\"year\", \"month\").agg(\n",
    "    F.sum(\"total_reviews\").alias(\"total_reviews\"),\n",
    "    (F.sum(F.col(\"avg_rating\") * F.col(\"total_reviews\")) / F.sum(\"total_reviews\")).alias(\"avg_rating\"),\n",
    "    F.sum(\"total_helpful_votes\").alias(\"total_helpful_votes\")\n",
    ")\n",
    "print(f\"Calculated Review Stats Monthly in {time.time() - start_calc_stats:.2f} seconds.\")\n",
    "print(f\"--- [Calculating Review Stats Monthly] Finished ---\")\n",
    "\n",
    "\n",
    "print(f\"\\n--- [Writing Review Stats Monthly Table] Starting ---\")\n",
    "start_write_stats = time.time()\n",
    "(df_review_stats_monthly.write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(HDFS_REVIEW_STATS_MONTHLY_DIR)\n",
    ")\n",
    "print(f\"Successfully wrote Review Stats Monthly Parquet data in {time.time() - start_write_stats:.2f} seconds.\")\n",
    "print(f\"--- [Writing Review Stats Monthly Table] Finished ---\")\n",
    "\n",
    "# Hive Table Review Stats Monthly\n",
    "print(f\"\\n--- [Creating Hive Review Stats Monthly Table] Starting ---\")\n",
    "start_hive_stats = time.time()\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_CURATED_DB}.{HIVE_REVIEW_STATS_MONTHLY_TABLE}\")\n",
    "spark.sql(f\"DROP VIEW IF EXISTS {HIVE_CURATED_DB}.{HIVE_MONTHLY_STATS_VIEW_OLD}\")\n",
    "create_stats_monthly_table_sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {HIVE_CURATED_DB}.{HIVE_REVIEW_STATS_MONTHLY_TABLE} (\n",
    "        year INT, month INT, total_reviews BIGINT, avg_rating DOUBLE,\n",
    "        total_helpful_votes BIGINT\n",
    "    ) STORED AS PARQUET LOCATION '{HDFS_REVIEW_STATS_MONTHLY_DIR}'\n",
    "    TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "spark.sql(create_stats_monthly_table_sql)\n",
    "print(f\"Successfully created Review Stats Monthly table in {time.time() - start_hive_stats:.2f} seconds.\")\n",
    "print(f\"--- [Creating Hive Review Stats Monthly Table] Finished ---\")\n",
    "\n",
    "\n",
    "print(\"\\n--- [Cleanup] Starting ---\")\n",
    "try:\n",
    "    if 'df_metadata' in locals() and df_metadata.is_cached:\n",
    "        print(\"Unpersisting Metadata DataFrame...\")\n",
    "        df_metadata.unpersist()\n",
    "    if 'df_base_curated' in locals() and df_base_curated.is_cached:\n",
    "        print(\"Unpersisting Base Curated DataFrame...\")\n",
    "        df_base_curated.unpersist()\n",
    "except Exception as unpersist_err:\n",
    "     print(f\"Warning: Error unpersisting data - {unpersist_err}\")\n",
    "print(\"--- [Cleanup] Finished ---\")\n",
    "\n",
    "\n",
    "end_time_job = time.time()\n",
    "print(\"\\n==========================================================\")\n",
    "print(f\"=== Execution Completed Successfully ===\")\n",
    "print(f\"=== Total Time: {end_time_job - start_time_job:.2f} seconds ({ (end_time_job - start_time_job)/60 :.2f} minutes) ===\")\n",
    "print(\"==========================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d067b",
   "metadata": {},
   "source": [
    "## TEST CASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee08e55a-c06c-4d66-afab-bfd5620d13c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+\n",
      "|       main_category|product_count|\n",
      "+--------------------+-------------+\n",
      "|        Buy a Kindle|      1560072|\n",
      "|                    |        31199|\n",
      "|                null|           92|\n",
      "|Magazine Subscrip...|            5|\n",
      "|            Software|            3|\n",
      "+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\"SELECT COUNT(DISTINCT parent_asin) AS distinct_parent_asin_count FROM curated.kindle_reviews_base\",\n",
    "           \"SELECT COUNT(*) AS parent_summary_count FROM curated.parent_product_overall_summary_materialized\",\n",
    "           \"SELECT COUNT(DISTINCT year, month) AS distinct_year_month_count FROM curated.kindle_reviews_base\",\n",
    "           \"SELECT COUNT(*) AS monthly_stats_count FROM curated.review_stats_monthly\",\n",
    "           \"SELECT COUNT(*) AS silver_count FROM processed.kindle_reviews_main\", \n",
    "           \"SELECT COUNT(*) AS base_curated_count FROM curated.kindle_reviews_base\"]\n",
    "for q in queries:\n",
    "    spark.sql(q).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beec0be7-8985-4e28-8a19-192f9cbae3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|main_category|            cat_path|\n",
      "+-------------+--------------------+\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|         null|Kindle Store > Ki...|\n",
      "|             |Kindle Store > Ki...|\n",
      "|             |Kindle Store > Ki...|\n",
      "|             |Kindle Store > Ki...|\n",
      "|             |Kindle Store > Ki...|\n",
      "|             |Kindle Store > Ki...|\n",
      "|             |Kindle Store > Ki...|\n",
      "+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def test_partition_filtering_performance(spark: SparkSession,\n",
    "                                         table_name: str,\n",
    "                                         specific_year: int,\n",
    "                                         specific_month: int,\n",
    "                                         some_asin: str):\n",
    "    print(f\"--- Starting Test Case TC4aM_18: Partition Filtering Performance ---\")\n",
    "    print(f\"Testing table: {table_name}\")\n",
    "    print(f\"Partition filter: year={specific_year}, month={specific_month}\")\n",
    "    print(f\"Non-partition filter: asin='{some_asin}'\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    t1 = -1.0\n",
    "    t2 = -1.0\n",
    "    count1 = -1\n",
    "    count2 = -1\n",
    "\n",
    "    query1 = f\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM {table_name}\n",
    "        WHERE month = {specific_month} \n",
    "    \"\"\"\n",
    "    print(f\"Executing Query 1 (Partition Filtered):\\n{query1}\")\n",
    "    try:\n",
    "        start_time_1 = time.time()\n",
    "        result1 = spark.sql(query1).first()\n",
    "        t1 = time.time() - start_time_1\n",
    "        count1 = result1[0] if result1 else 0\n",
    "        print(f\"Query 1 completed in: {t1:.4f} seconds\")\n",
    "        print(f\"Result Count 1: {count1}\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! ERROR executing Query 1: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    query2 = f\"\"\"\n",
    "        SELECT COUNT(*)\n",
    "        FROM {table_name}\n",
    "        WHERE asin = '{some_asin}'\n",
    "    \"\"\"\n",
    "    print(f\"Executing Query 2 (Non-Partition Filtered):\\n{query2}\")\n",
    "    try:\n",
    "        start_time_2 = time.time()\n",
    "        result2 = spark.sql(query2).first()\n",
    "        t2 = time.time() - start_time_2\n",
    "        count2 = result2[0] if result2 else 0\n",
    "        print(f\"Query 2 completed in: {t2:.4f} seconds\")\n",
    "        print(f\"Result Count 2: {count2}\")\n",
    "    except Exception as e:\n",
    "        print(f\"!!! ERROR executing Query 2: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "\n",
    "    print(\"--- Test Result Comparison ---\")\n",
    "    if t1 >= 0 and t2 >= 0: \n",
    "        print(f\"Time with Partition Filter (T1): {t1:.4f} seconds\")\n",
    "        print(f\"Time without Partition Filter (T2): {t2:.4f} seconds\")\n",
    "\n",
    "        if t1 < t2:\n",
    "            speedup_factor = t2 / t1 if t1 > 0 else float('inf')\n",
    "            print(f\"\\nPASSED: Query with partition filter (T1) is faster than without (T2).\")\n",
    "            print(f\"Speedup Factor (T2/T1): {speedup_factor:.2f}x (Higher is better)\")\n",
    "            print(\"Conclusion: Partition pruning appears to be working effectively.\")\n",
    "        elif t1 == t2:\n",
    "             print(f\"\\nWARNING: Query times are equal (T1 == T2).\")\n",
    "             print(\"This might happen if:\")\n",
    "             print(\"  - The table is very small.\")\n",
    "             print(f\"  - The specific ASIN '{some_asin}' only exists within the partition year={specific_year}, month={specific_month}.\")\n",
    "             print(\"  - Partition pruning might not be as effective as expected in this specific case.\")\n",
    "        else: # t1 > t2\n",
    "            print(f\"\\nFAILED: Query with partition filter (T1) is SLOWER than without (T2).\")\n",
    "            print(\"This is unexpected and indicates a potential issue with partitioning, statistics, or the query plan.\")\n",
    "            print(\"Please investigate the query plans and table statistics.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nSKIPPED COMPARISON: One or both queries failed to execute.\")\n",
    "\n",
    "    print(f\"--- Finished Test Case TC4aM_18 ---\")\n",
    "\n",
    "\n",
    "\n",
    "TARGET_TABLE = \"curated.kindle_reviews_base\"\n",
    "TEST_YEAR = 2014\n",
    "TEST_MONTH = 6\n",
    "TEST_ASIN = \"B000FA5KKA\"\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"SELECT 1 FROM {TARGET_TABLE} LIMIT 1\").collect()\n",
    "    print(f\"Table {TARGET_TABLE} found. Proceeding with test...\")\n",
    "\n",
    "    test_partition_filtering_performance(spark, TARGET_TABLE, TEST_YEAR, TEST_MONTH, TEST_ASIN)\n",
    "except Exception as setup_err:\n",
    "    print(f\"Error accessing table {TARGET_TABLE}. Cannot run test. Error: {setup_err}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a55b10d9-68e8-4939-a093-8e37f201377a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|main_category|product_count|\n",
      "+-------------+-------------+\n",
      "| Buy a Kindle|      1560072|\n",
      "|             |        31199|\n",
      "|         null|           92|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT main_category, COUNT(DISTINCT parent_asin) AS product_count\n",
    "    FROM curated.item_metadata\n",
    "    GROUP BY main_category\n",
    "    HAVING product_count > 50\n",
    "    ORDER BY product_count DESC;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c6391bfd-3b3d-42c3-a4c0-09a6e40f5997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+----------+\n",
      "|main_category|            cat_path|path_count|\n",
      "+-------------+--------------------+----------+\n",
      "|         null|Kindle Store > Ki...|        32|\n",
      "|         null|Kindle Store > Ki...|         6|\n",
      "|         null|Kindle Store > Ki...|         5|\n",
      "|         null|Kindle Store > Ki...|         5|\n",
      "|         null|Kindle Store > Ki...|         5|\n",
      "|         null|Kindle Store > Ki...|         4|\n",
      "|         null|Kindle Store > Ki...|         4|\n",
      "|         null|Kindle Store > Ki...|         3|\n",
      "|         null|Kindle Store > Ki...|         3|\n",
      "|         null|Kindle Store > Ki...|         3|\n",
      "|         null|Kindle Store > Ki...|         3|\n",
      "|         null|Kindle Store > Ki...|         3|\n",
      "|         null|Kindle Store > Ki...|         2|\n",
      "|         null|Kindle Store > Ki...|         2|\n",
      "|             |Kindle Store > Ki...|     26300|\n",
      "|             |Kindle Store > Ki...|      1951|\n",
      "|             |Kindle Store > Ki...|       796|\n",
      "|             |Kindle Store > Ki...|       742|\n",
      "|             |Kindle Store > Ki...|       416|\n",
      "|             |Kindle Store > Ki...|       402|\n",
      "+-------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    WITH cat_path_count AS (\n",
    "      SELECT main_category, cat_path, COUNT(*) AS path_count\n",
    "      FROM curated.item_metadata\n",
    "      GROUP BY main_category, cat_path\n",
    "    )\n",
    "    SELECT main_category, cat_path, path_count\n",
    "    FROM cat_path_count\n",
    "    WHERE path_count > 1\n",
    "    ORDER BY main_category, path_count DESC;\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4535b997-fe8b-4202-8139-f783216eb8ce",
   "metadata": {},
   "source": [
    "## Sentiment Analysis and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ca74a2-6285-4a08-a3a2-54643272d22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 4: Data Enrichment (DistilBERT + LDA) ---\n",
      "Target Hardware Profile: 4 Cores, 16GB RAM\n",
      "Sentiment Model: sentiment_tinybert\n",
      "LDA Topics: 10, Vocab Size: 5000\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "HIVE_CURATED_DB = \"curated\"\n",
    "HIVE_CURATED_BASE_TABLE = \"kindle_reviews_base\"\n",
    "START_YEAR = 2023\n",
    "END_YEAR = 2023\n",
    "\n",
    "def count_records_in_years(spark: SparkSession, db_name: str, table_name: str, start_year: int, end_year: int) -> int:\n",
    "    \"\"\"\n",
    "    Đếm số lượng bản ghi trong một bảng Hive trong khoảng năm nhất định.\n",
    "    Sử dụng partition pruning hiệu quả.\n",
    "    \"\"\"\n",
    "    print(f\"--- Counting records in {db_name}.{table_name} from year {start_year} to {end_year} ---\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "\n",
    "        count_query = f\"\"\"\n",
    "        SELECT COUNT(*) as record_count\n",
    "        FROM {db_name}.{table_name}\n",
    "        WHERE year >= {start_year} AND year <= {end_year}\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"Executing query: {count_query}\")\n",
    "        count_result_df = spark.sql(count_query)\n",
    "\n",
    "        count_value = count_result_df.first()[\"record_count\"]\n",
    "\n",
    "        print(f\"Query executed successfully.\")\n",
    "        print(f\"Counting took {time.time() - start_time:.2f} seconds.\")\n",
    "        return count_value\n",
    "    except Exception as e:\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"!!! ERROR during record counting: {e}\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        traceback.print_exc()\n",
    "        return -1\n",
    "\n",
    "def run_record_count_check():\n",
    "\n",
    "    start_time_job = time.time()\n",
    "    print(\"======================================================\")\n",
    "    print(\"=== Starting Job: Count Records for Recent Years ===\")\n",
    "    print(\"======================================================\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        record_count = count_records_in_years(spark, HIVE_CURATED_DB, HIVE_CURATED_BASE_TABLE, START_YEAR, END_YEAR)\n",
    "\n",
    "        if record_count >= 0:\n",
    "            print(\"\\n------------------------------------------------------\")\n",
    "            print(f\"RESULT: Found {record_count:,} records between {START_YEAR} and {END_YEAR} (inclusive).\")\n",
    "            print(\"------------------------------------------------------\")\n",
    " \n",
    "            target_sample_size = 2_000_000\n",
    "            percentage_of_target = (record_count / target_sample_size) * 100 if target_sample_size > 0 else 0\n",
    "            print(f\"This represents roughly {percentage_of_target:.2f}% of the target sample size (~2 million).\")\n",
    "            print(\"\\nNext Step Considerations:\")\n",
    "            if abs(record_count - target_sample_size) / target_sample_size <= 0.25: \n",
    "                 print(f\"-> The record count is close to the target. Consider using years {START_YEAR}-{END_YEAR} directly for Job 4b.\")\n",
    "            elif record_count < target_sample_size:\n",
    "                 print(f\"-> The record count is significantly smaller than the target. Consider expanding the year range (e.g., start from 2016 or earlier) or combining with sampling from older years.\")\n",
    "            else:\n",
    "                 print(f\"-> The record count is significantly larger than the target. Consider narrowing the year range (e.g., 2019-2023) or applying additional sampling (e.g., `.sample(fraction=...)`) on the {START_YEAR}-{END_YEAR} data.\")\n",
    "\n",
    "        else:\n",
    "            print(\"\\nError occurred during counting. Please check the logs.\")\n",
    "\n",
    "\n",
    "        end_time_job = time.time()\n",
    "        print(\"\\n======================================================\")\n",
    "        print(f\"=== Job Execution Finished ===\")\n",
    "        print(f\"=== Total Time: {end_time_job - start_time_job:.2f} seconds ===\")\n",
    "        print(\"======================================================\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"!!! FATAL ERROR occurred during job execution !!!\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        traceback.print_exc()\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    run_record_count_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e338ddf0-532e-440a-a730-7a39cfa4ec48",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 2: Loading Data from Silver Layer ---\n",
      "Reading from Hive table: processed.kindle_reviews_main\n",
      "Schema of loaded data:\n",
      "root\n",
      " |-- rating: float (nullable = true)\n",
      " |-- title_processed: string (nullable = true)\n",
      " |-- text_processed: string (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      " |-- helpful_vote: integer (nullable = true)\n",
      " |-- review_time: timestamp (nullable = true)\n",
      " |-- date_str: string (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n",
      "Successfully loaded data schema.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (StringType, ArrayType, StructType,\n",
    "                               StructField, FloatType, IntegerType, LongType,\n",
    "                               BooleanType, TimestampType, MapType, DoubleType)\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "from pyspark.ml import Pipeline as SparkMLPipeline\n",
    "\n",
    "import sparknlp\n",
    "from sparknlp.base import *\n",
    "from sparknlp.annotator import *\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer, IDF\n",
    "from pyspark.ml.clustering import LDA, LDAModel\n",
    "\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "HIVE_CURATED_DB = \"curated\"\n",
    "\n",
    "\n",
    "HIVE_CURATED_BASE_TABLE = \"kindle_reviews_base\"\n",
    "\n",
    "# Output Enriched Sample Table\n",
    "HDFS_CURATED_ENRICHED_SAMPLE_DIR = \"hdfs:///data/curated/amazon_reviews/kindle_store_enriched_sample\"\n",
    "HIVE_CURATED_ENRICHED_SAMPLE_TABLE = \"kindle_reviews_enriched_sample\"\n",
    "\n",
    "# --- Sampling Parameters ---\n",
    "INCLUDE_YEARS = [2023, 2022]\n",
    "FILTER_3_STAR = True\n",
    "RATINGS_TO_KEEP = [1.0, 2.0, 4.0, 5.0]\n",
    "MIN_REVIEWS_THRESHOLD = 20 # Ngưỡng cho parent_asin\n",
    "\n",
    "\n",
    "SENTIMENT_MODEL_NAME = \"sentimentdl_use_imdb\"\n",
    "# LDA Parameters\n",
    "NUM_TOPICS = 15 \n",
    "LDA_MAX_ITER = 20\n",
    "LDA_VOCAB_SIZE = 10000 \n",
    "LDA_MIN_DF = 5 \n",
    "\n",
    "\n",
    "DEFAULT_TITLE = \"[Unknown Parent Title]\"\n",
    "DEFAULT_MAIN_CATEGORY = \"[Unknown Main Category]\"\n",
    "DEFAULT_CATEGORY = \"[Unknown Parent Category]\"\n",
    "DEFAULT_SENTIMENT = \"neutral\" # Default nếu model lỗi\n",
    "DEFAULT_TOPIC = -1\n",
    "DEFAULT_KEYWORDS = \"N/A\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfeb34d-5cf8-4b2d-9b35-eb61c0d7a1db",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 3: Sentiment Analysis (DistilBERT) ---\n",
      "Setting up DistilBERT pipeline using 'sentiment_tinybert'...\n",
      "ERROR during Sentiment Analysis: \n",
      "Possible causes: OutOfMemoryError (check Spark UI), incorrect model name, network issues.\n"
     ]
    }
   ],
   "source": [
    "spark = None\n",
    "start_time_job = time.time()\n",
    "print(\"====================================================================\")\n",
    "print(\"=== Starting Job 4b: Create Enriched Sample Table (Sentiment+Topic) ===\")\n",
    "print(\"====================================================================\")\n",
    "\n",
    "\n",
    "spark_nlp_version = \"5.5.3\" \n",
    "scala_version = \"2.12\" \n",
    "\n",
    "hadoop_aws_version = \"3.3.1\" \n",
    "aws_sdk_version = \"1.12.262\" \n",
    "\n",
    "packages = [\n",
    "    f\"com.johnsnowlabs.nlp:spark-nlp_{scala_version}:{spark_nlp_version}\",\n",
    "    f\"org.apache.hadoop:hadoop-aws:{hadoop_aws_version}\",\n",
    "    f\"com.amazonaws:aws-java-sdk-bundle:{aws_sdk_version}\" \n",
    "]\n",
    "packages_str = \",\".join(packages)\n",
    "print(f\"Using packages: {packages_str}\")\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkNLP_AmazonReviews\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.driver.host\", \"jupyter\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.executor.instances\", \"9\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.executor.cores\", \"3\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.yarn.executor.memoryOverhead\", \"2g\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4G\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.default.parallelism\", \"30\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"30\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"256m\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.jars.packages\", packages_str) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n--- SparkSession Initialized ---\")\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "try:\n",
    "    print(f\"Spark NLP version: {sparknlp.version()}\") \n",
    "except Exception as e:\n",
    "    print(f\"Could not get Spark NLP version via sparknlp.version(): {e}\")\n",
    "    \n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"Deploy Mode: {spark.sparkContext.getConf().get('spark.submit.deployMode')}\")\n",
    "print(f\"Spark Packages Config: {spark.sparkContext.getConf().get('spark.jars.packages')}\") # Check packages\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f747419-5f6c-4b57-bb1b-7a5be3e4969f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"\\n--- [Reading Base Curated Table] Starting ---\")\n",
    "start_read_base = time.time()\n",
    "df_base_read = spark.table(f\"{HIVE_CURATED_DB}.{HIVE_CURATED_BASE_TABLE}\")\n",
    "print(f\"Finished reading Base Curated data in {time.time() - start_read_base:.2f} seconds.\")\n",
    "print(f\"--- [Reading Base Curated Table] Finished ---\")\n",
    "\n",
    "# 2. Áp dụng Logic Lấy Mẫu Có Chủ Đích\n",
    "print(f\"\\n--- [Applying Purposeful Sampling Logic] Starting ---\")\n",
    "start_sampling = time.time()\n",
    "\n",
    "# 2.1 Lọc theo năm\n",
    "print(f\"Filtering by years: {INCLUDE_YEARS}\")\n",
    "df_filtered_time = df_base_read.filter(F.col(\"year\").isin(INCLUDE_YEARS))\n",
    "\n",
    "# 2.2 Lọc theo rating \n",
    "if FILTER_3_STAR:\n",
    "    print(f\"Filtering ratings to keep: {RATINGS_TO_KEEP}\")\n",
    "    df_filtered_rating = df_filtered_time.filter(F.col(\"rating\").isin(RATINGS_TO_KEEP))\n",
    "else:\n",
    "    print(\"Keeping all ratings.\")\n",
    "    df_filtered_rating = df_filtered_time\n",
    "    \n",
    "    \n",
    "\n",
    "df_filtered_rating.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "filtered_rating_count = df_filtered_rating.count()\n",
    "print(f\"Count after time and rating filter: {filtered_rating_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5a5513-3fac-4ec4-bd24-9497fdf64130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Tính toán số lượng review theo parent_asin\n",
    "print(\"Calculating review counts per parent_asin on filtered data...\")\n",
    "parent_counts = df_filtered_rating.groupBy(\"parent_asin\") \\\n",
    "                                 .agg(F.count(\"*\").alias(\"review_count\"))\n",
    "\n",
    "# 2.4 Xác định parent_asin đủ điều kiện\n",
    "print(f\"Identifying parent_asins with >= {MIN_REVIEWS_THRESHOLD} reviews...\")\n",
    "eligible_parents = parent_counts.filter(F.col(\"review_count\") >= MIN_REVIEWS_THRESHOLD) \\\n",
    "                                .select(\"parent_asin\")\n",
    "eligible_parent_count = eligible_parents.count()\n",
    "print(f\"Found {eligible_parent_count} eligible parent_asins.\")\n",
    "\n",
    "# 2.5 Lọc dữ liệu cuối cùng bằng semi join\n",
    "print(\"Filtering final sample using semi join...\")\n",
    "df_sample = df_filtered_rating.join(\n",
    "    F.broadcast(eligible_parents), \n",
    "    on=\"parent_asin\",\n",
    "    how=\"semi\"\n",
    ")\n",
    "\n",
    "df_sample.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "final_sample_count = df_sample.count() # Trigger\n",
    "\n",
    "\n",
    "df_filtered_rating.unpersist()\n",
    "\n",
    "print(f\"Finished purposeful sampling. Final sample size: {final_sample_count}\")\n",
    "print(f\"Sampling took {time.time() - start_sampling:.2f} seconds.\")\n",
    "print(f\"Schema of the sample:\")\n",
    "df_sample.printSchema()\n",
    "print(f\"--- [Applying Purposeful Sampling Logic] Finished ---\")\n",
    "\n",
    "\n",
    "if final_sample_count == 0:\n",
    "    print(\"WARNING: Sample size is 0! Check filtering logic and data. Stopping job.\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "elif final_sample_count < 1000:\n",
    "     print(f\"WARNING: Sample size ({final_sample_count}) is very small.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0baa4bb3-f989-46bd-8bea-7bbec70cd580",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing SparkSession via Packages (includes Spark NLP + AWS deps) ---\n",
      "Using packages: com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.3,org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\n",
      "\n",
      "--- SparkSession Initialized ---\n",
      "Spark version: 3.3.0\n",
      "Spark NLP version: 5.5.3\n",
      "Master: yarn\n",
      "Deploy Mode: client\n",
      "Spark Packages Config: com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.3,org.apache.hadoop:hadoop-aws:3.3.1,com.amazonaws:aws-java-sdk-bundle:1.12.262\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# 3. Sentiment Analysis \n",
    "print(f\"\\n--- [Sentiment Analysis (USE + sentimentdl_use_imdb)] Starting ---\")\n",
    "start_sentiment = time.time()\n",
    "\n",
    "document_assembler_sent = DocumentAssembler() \\\n",
    "    .setInputCol(\"text_processed\") \\\n",
    "    .setOutputCol(\"document_sent\")\n",
    "\n",
    "# Load Universal Sentence Encoder\n",
    "use_embeddings = UniversalSentenceEncoder.pretrained('tfhub_use', lang=\"en\") \\\n",
    "    .setInputCols([\"document_sent\"])\\\n",
    "    .setOutputCol(\"sentence_embeddings\")\n",
    "\n",
    "# Load SentimentDL model pretrained on IMDB using USE embeddings\n",
    "sentiment_classifier_use = SentimentDLModel().pretrained('sentimentdl_use_imdb', 'en') \\\n",
    "    .setInputCols([\"sentence_embeddings\"]) \\\n",
    "    .setOutputCol(\"sentiment_class_use\") \\\n",
    "    .setThreshold(0.6) \n",
    "\n",
    "sentiment_pipeline_use = SparkMLPipeline(stages=[\n",
    "    document_assembler_sent,\n",
    "    use_embeddings,\n",
    "    sentiment_classifier_use\n",
    "])\n",
    "\n",
    "print(\"Applying USE + SentimentDL pipeline to the sample...\")\n",
    "\n",
    "df_sentiment_result = sentiment_pipeline_use.fit(df_sample).transform(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063d1c6-4aab-4f73-a0b2-072f722743b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_sentiment_added = df_sentiment_result \\\n",
    "    .withColumn(\"sentiment_label_raw\", F.col(\"sentiment_class_use.result\")[0]) \\\n",
    "    .withColumn(\"sentiment_meta\", F.col(\"sentiment_class_use.metadata\")[0]) \\\n",
    "    .withColumn(\"sentiment_label\", \\\n",
    "        F.when(F.col(\"sentiment_label_raw\") == \"positive\", \"positive\") \\\n",
    "        .when(F.col(\"sentiment_label_raw\") == \"negative\", \"negative\") \\\n",
    "        .otherwise(DEFAULT_SENTIMENT)) \\\n",
    "    .withColumn(\"sentiment_score\", \\\n",
    "        F.when(F.col(\"sentiment_label\") == \"positive\", F.col(\"sentiment_meta.positive\").cast(DoubleType())) \\\n",
    "         .when(F.col(\"sentiment_label\") == \"negative\", F.col(\"sentiment_meta.negative\").cast(DoubleType())) \\\n",
    "         .when(F.col(\"sentiment_label\") == \"neutral\", F.col(\"sentiment_meta.neutral\").cast(DoubleType())) \\\n",
    "         .otherwise(0.0) \\\n",
    "         .alias(\"sentiment_score\", metadata={'confidence': True}))\n",
    "\n",
    "\n",
    "df_sentiment_processed = df_sentiment_added.select(\n",
    "    \"*\",\n",
    "    \"sentiment_label\",\n",
    "    \"sentiment_score\"\n",
    ").drop(\"document_sent\", \"sentence_embeddings\", \"sentiment_class_use\", \"sentiment_label_raw\", \"sentiment_meta\")\n",
    "\n",
    "\n",
    "df_sentiment_processed.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "sentiment_count = df_sentiment_processed.count()\n",
    "\n",
    "print(f\"Sentiment Analysis completed. Processed {sentiment_count} records.\")\n",
    "print(f\"Sentiment analysis took {time.time() - start_sentiment:.2f} seconds.\")\n",
    "print(f\"--- [Sentiment Analysis ({SENTIMENT_MODEL_NAME})] Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5aecbef-eccb-44f7-9b89-f0b65ff1b4a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up common NLP pipeline...\n",
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n",
      "stopwords_en download started this may take some time.\n",
      "Approximate size to download 2.9 KB\n",
      "[OK!]\n",
      "Applying common NLP pipeline...\n"
     ]
    }
   ],
   "source": [
    "# 4. Topic Modeling (LDA)\n",
    "print(f\"\\n--- [Topic Modeling (LDA k={NUM_TOPICS})] Starting ---\")\n",
    "start_topic = time.time()\n",
    "\n",
    "# 4.1. NLP Preprocessing for LDA\n",
    "print(\"Setting up NLP pipeline for LDA...\")\n",
    "document_assembler_lda = DocumentAssembler() \\\n",
    "    .setInputCol(\"text_processed\") \\\n",
    "    .setOutputCol(\"document_lda\")\n",
    "tokenizer_lda = Tokenizer() \\\n",
    "    .setInputCols([\"document_lda\"]) \\\n",
    "    .setOutputCol(\"token_lda\")\n",
    "normalizer_lda = Normalizer() \\\n",
    "    .setInputCols([\"token_lda\"]) \\\n",
    "    .setOutputCol(\"normalized_lda\") \\\n",
    "    .setLowercase(True) \\\n",
    "    .setCleanupPatterns([\"\"\"[^\\w\\d\\s]\"\"\"])\n",
    "lemmatizer_lda = LemmatizerModel.pretrained(\"lemma_antbnc\", \"en\") \\\n",
    "    .setInputCols([\"normalized_lda\"]) \\\n",
    "    .setOutputCol(\"lemma_lda\")\n",
    "stopwords_cleaner_lda = StopWordsCleaner.pretrained(\"stopwords_en\", \"en\") \\\n",
    "    .setInputCols([\"lemma_lda\"]) \\\n",
    "    .setOutputCol(\"clean_lemma_lda\") \\\n",
    "    .setCaseSensitive(False)\n",
    "finisher_lda = Finisher() \\\n",
    "    .setInputCols([\"clean_lemma_lda\"]) \\\n",
    "    .setOutputCols([\"finished_tokens_lda\"]) \\\n",
    "    .setOutputAsArray(True) \\\n",
    "    .setCleanAnnotations(False)\n",
    "nlp_pipeline_lda = SparkMLPipeline(stages=[\n",
    "    document_assembler_lda, tokenizer_lda, normalizer_lda,\n",
    "    lemmatizer_lda, stopwords_cleaner_lda, finisher_lda\n",
    "])\n",
    "print(\"Applying NLP pipeline for LDA...\")\n",
    "df_lda_tokens = nlp_pipeline_lda.fit(df_sentiment_processed).transform(df_sentiment_processed)\n",
    "df_lda_tokens = df_lda_tokens.filter(F.size(F.col(\"finished_tokens_lda\")) > 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd5f3a6-7ea8-4e76-96a0-4a1de7165787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying TF-IDF for Sentiment Analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/conda/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m idf \u001b[38;5;241m=\u001b[39m IDF(inputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrawFeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures_tfidf\u001b[39m\u001b[38;5;124m\"\u001b[39m, minDocFreq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;66;03m# minDocFreq loại bỏ từ quá hiếm\u001b[39;00m\n\u001b[1;32m      4\u001b[0m tfidf_pipeline \u001b[38;5;241m=\u001b[39m SparkMLPipeline(stages\u001b[38;5;241m=\u001b[39m[hashingTF, idf])\n\u001b[0;32m----> 5\u001b[0m tfidf_model \u001b[38;5;241m=\u001b[39m \u001b[43mtfidf_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_nlp_processed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m df_tfidf \u001b[38;5;241m=\u001b[39m tfidf_model\u001b[38;5;241m.\u001b[39mtransform(df_nlp_processed)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/pipeline.py:134\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    132\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m stage\u001b[38;5;241m.\u001b[39mtransform(dataset)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mstage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     transformers\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m indexOfLastEstimator:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/wrapper.py:379\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 379\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/ml/wrapper.py:376\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4.2. CountVectorizer\n",
    "print(f\"Applying CountVectorizer (vocabSize={LDA_VOCAB_SIZE}, minDF={LDA_MIN_DF})...\")\n",
    "cv = CountVectorizer(inputCol=\"finished_tokens_lda\", outputCol=\"features_countvec\",\n",
    "                     vocabSize=LDA_VOCAB_SIZE, minDF=LDA_MIN_DF)\n",
    "cv_model = cv.fit(df_lda_tokens)\n",
    "df_vectorized_lda = cv_model.transform(df_lda_tokens)\n",
    "vocabulary = cv_model.vocabulary\n",
    "print(f\"Actual vocabulary size: {len(vocabulary)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95458549-8f27-45fb-a053-f820900220a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3. LDA Training & Transform\n",
    "print(f\"Training LDA model (k={NUM_TOPICS}, maxIter={LDA_MAX_ITER})...\")\n",
    "lda = LDA(k=NUM_TOPICS, maxIter=LDA_MAX_ITER, featuresCol=\"features_countvec\",\n",
    "          seed=42, optimizer=\"online\")\n",
    "lda_model: LDAModel = lda.fit(df_vectorized_lda) \n",
    "print(\"Applying LDA model to get topic distributions...\")\n",
    "df_topics = lda_model.transform(df_vectorized_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620efba6-0b9c-4028-9f53-5e4cf9345c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.4. Extract Dominant Topic and Keywords\n",
    "print(\"Extracting dominant topic and keywords...\")\n",
    "\n",
    "@F.udf(returnType=IntegerType())\n",
    "def get_dominant_topic_udf(topic_dist):\n",
    "    if topic_dist is None: return DEFAULT_TOPIC\n",
    "    return int(topic_dist.argmax())\n",
    "\n",
    "df_topics = df_topics.withColumn(\"dominant_topic\", get_dominant_topic_udf(F.col(\"topicDistribution\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c46a66-c9c9-495f-b84b-25246ef61001",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################         Lấy keywords\n",
    "topic_indices = lda_model.describeTopics(maxTermsPerTopic=7)\n",
    "topic_keywords_map = {}\n",
    "for row in topic_indices.collect():\n",
    "    topic_id = row['topic']\n",
    "    term_indices = row['termIndices']\n",
    "    keywords = [vocabulary[i] for i in term_indices]\n",
    "    topic_keywords_map[topic_id] = \", \".join(keywords)\n",
    "\n",
    "# Tạo DataFrame lookup và join\n",
    "topic_keywords_list = list(topic_keywords_map.items())\n",
    "if not topic_keywords_list: \n",
    "    print(\"WARNING: No topics described by LDA model!\")\n",
    "    df_enriched_sample = df_topics.withColumn(\"topic_keywords\", F.lit(DEFAULT_KEYWORDS))\n",
    "else:\n",
    "    df_topic_keywords = spark.createDataFrame(topic_keywords_list, [\"dominant_topic_ref\", \"topic_keywords\"])\n",
    "    df_enriched_sample = df_topics.join(\n",
    "        F.broadcast(df_topic_keywords), \n",
    "        df_topics[\"dominant_topic\"] == df_topic_keywords[\"dominant_topic_ref\"],\n",
    "        \"left\"\n",
    "    ).drop(\"dominant_topic_ref\") \\\n",
    "     .fillna({\"topic_keywords\": DEFAULT_KEYWORDS}) \n",
    "\n",
    "print(\"Topic Modeling and Keyword Mapping completed.\")\n",
    "df_enriched_sample.select(\"text_processed\", \"dominant_topic\", \"topic_keywords\").show(5, truncate=80)\n",
    "print(f\"Topic modeling took {time.time() - start_topic:.2f} seconds.\")\n",
    "print(f\"--- [Topic Modeling (LDA k={NUM_TOPICS})] Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140070fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched_sample.select(\"text_processed\", \"dominant_topic\", \"topic_keywords\").show(5, truncate=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc20f0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Select Final Columns for Enriched Sample Table\n",
    "print(f\"\\n--- [Selecting Final Columns for Enriched Sample] Starting ---\")\n",
    "df_final_enriched = df_enriched_sample.select(\n",
    "    # Cột gốc từ Base\n",
    "    \"parent_asin\", \"asin\", \"user_id\", \"review_time\",\n",
    "    \"year\", \"month\", \"date_str\", \"rating\", \"helpful_vote\",\n",
    "    \"verified_purchase\", \"text_processed\", \"parent_title\", \"main_category\",\n",
    "    \"parent_category\", \"images\",\n",
    "    # Cột làm giàu\n",
    "    F.coalesce(F.col(\"sentiment_label\"), F.lit(DEFAULT_SENTIMENT)).alias(\"sentiment_label\"),\n",
    "    F.coalesce(F.col(\"sentiment_score\"), F.lit(0.0)).alias(\"sentiment_score\"),\n",
    "    F.coalesce(F.col(\"dominant_topic\"), F.lit(DEFAULT_TOPIC)).alias(\"dominant_topic\"),\n",
    "    F.coalesce(F.col(\"topic_keywords\"), F.lit(DEFAULT_KEYWORDS)).alias(\"topic_keywords\")\n",
    ")\n",
    "print(\"Final Schema for Enriched Sample Table:\")\n",
    "df_final_enriched.printSchema()\n",
    "print(f\"--- [Selecting Final Columns for Enriched Sample] Finished ---\")\n",
    "\n",
    "\n",
    "print(f\"\\n--- [Writing Enriched Sample Table] Starting ---\")\n",
    "print(f\"Writing Enriched Sample data to: {HDFS_CURATED_ENRICHED_SAMPLE_DIR}\")\n",
    "start_write_enriched = time.time()\n",
    "(df_final_enriched.write\n",
    "    .partitionBy(\"year\", \"month\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(HDFS_CURATED_ENRICHED_SAMPLE_DIR)\n",
    ")\n",
    "print(f\"Successfully wrote Enriched Sample Parquet data in {time.time() - start_write_enriched:.2f} seconds.\")\n",
    "print(f\"--- [Writing Enriched Sample Table] Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6977d14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Hive External Table cho Enriched Sample\n",
    "print(f\"\\n--- [Creating Hive Enriched Sample Table] Starting ---\")\n",
    "start_hive_enriched = time.time()\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {HIVE_CURATED_DB}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_CURATED_DB}.{HIVE_CURATED_ENRICHED_SAMPLE_TABLE}\")\n",
    "\n",
    "create_enriched_sample_table_sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {HIVE_CURATED_DB}.{HIVE_CURATED_ENRICHED_SAMPLE_TABLE} (\n",
    "        parent_asin STRING, asin STRING, user_id STRING, review_time TIMESTAMP,\n",
    "        date_str STRING, rating FLOAT, helpful_vote INT, verified_purchase BOOLEAN,\n",
    "        text_processed STRING, parent_title STRING, main_category STRING,\n",
    "        parent_category STRING, images ARRAY<STRING>,\n",
    "        sentiment_label STRING, sentiment_score DOUBLE,\n",
    "        dominant_topic INT, topic_keywords STRING\n",
    "    )\n",
    "    PARTITIONED BY (year INT, month INT)\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '{HDFS_CURATED_ENRICHED_SAMPLE_DIR}'\n",
    "    TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "spark.sql(create_enriched_sample_table_sql)\n",
    "print(f\"Table created. Running MSCK REPAIR TABLE for {HIVE_CURATED_ENRICHED_SAMPLE_TABLE}...\")\n",
    "spark.sql(f\"MSCK REPAIR TABLE {HIVE_CURATED_DB}.{HIVE_CURATED_ENRICHED_SAMPLE_TABLE}\")\n",
    "\n",
    "print(f\"Successfully created Enriched Sample table in {time.time() - start_hive_enriched:.2f} seconds.\")\n",
    "print(f\"--- [Creating Hive Enriched Sample Table] Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebbf892",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- [Cleanup] Starting ---\")\n",
    "try:\n",
    "    if 'df_sample' in locals() and df_sample.is_cached:\n",
    "        print(\"Unpersisting Sample DataFrame...\")\n",
    "        df_sample.unpersist()\n",
    "    if 'df_sentiment_processed' in locals() and df_sentiment_processed.is_cached:\n",
    "        print(\"Unpersisting Sentiment Processed DataFrame...\")\n",
    "        df_sentiment_processed.unpersist()\n",
    "\n",
    "except Exception as unpersist_err:\n",
    "     print(f\"Warning: Error unpersisting data - {unpersist_err}\")\n",
    "print(\"--- [Cleanup] Finished ---\")\n",
    "\n",
    "end_time_job = time.time()\n",
    "print(\"\\n====================================================================\")\n",
    "print(f\"=== Job 4b (Enrich Sample) Execution Completed Successfully ===\")\n",
    "print(f\"=== Total Time: {end_time_job - start_time_job:.2f} seconds ({ (end_time_job - start_time_job)/60 :.2f} minutes) ===\")\n",
    "print(f\"=== Final Sample Size Written: {final_sample_count} ===\")\n",
    "print(\"====================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dafff79",
   "metadata": {},
   "source": [
    "## OPTIMIZE WAREHOUSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff30c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVE_CURATED_DB = \"curated\"\n",
    "\n",
    "\n",
    "HDFS_CURATED_ENRICHED_SAMPLE_DIR = \"hdfs:///data/curated/amazon_reviews/kindle_store_enriched_sample\"\n",
    "\n",
    "HDFS_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_DIR = \"hdfs:///data/curated/amazon_reviews/monthly_sentiment_summary_sample\"\n",
    "HIVE_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_TABLE = \"monthly_sentiment_summary_sample\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n--- [Reading Stored Enriched Sample Data] Starting ---\")\n",
    "print(f\"Reading Parquet data from: {HDFS_CURATED_ENRICHED_SAMPLE_DIR}\")\n",
    "start_read_enriched = time.time()\n",
    "\n",
    "try:\n",
    "    df_final_enriched_loaded = spark.read.parquet(HDFS_CURATED_ENRICHED_SAMPLE_DIR)\n",
    "    enriched_count = df_final_enriched_loaded.count() \n",
    "    print(f\"Successfully read {enriched_count} records from stored enriched sample.\")\n",
    "    print(f\"Schema of loaded data:\")\n",
    "    df_final_enriched_loaded.printSchema()\n",
    "except Exception as read_err:\n",
    "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(f\"!!! ERROR Reading enriched data from {HDFS_CURATED_ENRICHED_SAMPLE_DIR}: {read_err}\")\n",
    "    print(f\"!!! Please ensure Job 4b ran successfully and data exists at this path.\")\n",
    "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    traceback.print_exc()\n",
    "    raise read_err\n",
    "\n",
    "print(f\"Reading stored enriched data took {time.time() - start_read_enriched:.2f} seconds.\")\n",
    "print(f\"--- [Reading Stored Enriched Sample Data] Finished ---\")\n",
    "\n",
    "\n",
    "# --- Materialize Monthly Sentiment Summary ---\n",
    "print(f\"\\n--- [Materializing Monthly Sentiment Summary] Starting ---\")\n",
    "start_mat_monthly_sent = time.time()\n",
    "\n",
    "# 2. Tính toán monthly sentiment counts\n",
    "print(\"Calculating monthly sentiment counts...\")\n",
    "df_monthly_sentiment_summary = df_final_enriched_loaded.groupBy(\"year\", \"month\", \"sentiment_label\").agg(\n",
    "    F.count(\"*\").alias(\"review_count\")\n",
    ")\n",
    "\n",
    "\n",
    "df_monthly_sentiment_summary.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "monthly_sent_count = df_monthly_sentiment_summary.count()\n",
    "print(f\"Calculated Monthly Sentiment Summary (Count: {monthly_sent_count})\")\n",
    "\n",
    "# 3. Ghi bảng materialized vào HDFS (Partition theo Year)\n",
    "print(f\"Writing Materialized Monthly Sentiment Summary to: {HDFS_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_DIR}\")\n",
    "start_write_monthly_sent = time.time()\n",
    "(df_monthly_sentiment_summary.write\n",
    "    .partitionBy(\"year\")\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(HDFS_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_DIR)\n",
    ")\n",
    "print(f\"Successfully wrote Materialized Monthly Sentiment Summary in {time.time() - start_write_monthly_sent:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20de6888",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Tạo Hive Table cho bảng materialized\n",
    "print(f\"Creating Hive table: {HIVE_CURATED_DB}.{HIVE_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_TABLE}\")\n",
    "start_hive_monthly_sent = time.time()\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {HIVE_CURATED_DB}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_CURATED_DB}.{HIVE_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_TABLE}\")\n",
    "\n",
    "create_monthly_sent_summary_table_sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE {HIVE_CURATED_DB}.{HIVE_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_TABLE} (\n",
    "        month INT,\n",
    "        sentiment_label STRING,\n",
    "        review_count BIGINT\n",
    "    )\n",
    "    PARTITIONED BY (year INT)\n",
    "    STORED AS PARQUET LOCATION '{HDFS_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_DIR}'\n",
    "    TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "spark.sql(create_monthly_sent_summary_table_sql)\n",
    "print(f\"Table created. Running MSCK REPAIR TABLE for {HIVE_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_TABLE}...\")\n",
    "spark.sql(f\"MSCK REPAIR TABLE {HIVE_CURATED_DB}.{HIVE_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_TABLE}\")\n",
    "print(f\"Successfully created/repaired {HIVE_MONTHLY_SENTIMENT_SUMMARY_SAMPLE_TABLE} in {time.time() - start_hive_monthly_sent:.2f} seconds.\")\n",
    "\n",
    "df_monthly_sentiment_summary.unpersist()\n",
    "\n",
    "print(f\"Materializing Monthly Sentiment Summary took {time.time() - start_mat_monthly_sent:.2f} seconds.\")\n",
    "print(f\"--- [Materializing Monthly Sentiment Summary] Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bf68d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c846fef",
   "metadata": {},
   "source": [
    "## MIGRATE DATA FROM HIVE TO POSTGRES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0423e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_spark()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"SparkNLP_AmazonReviews_YARN_Packages\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.driver.host\", \"jupyter\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", \"false\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"1000M\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2G\") \\\n",
    "    .config(\"spark.default.parallelism\", \"10\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", \"false\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.jars.packages\", 'org.postgresql:postgresql:42.7.3') \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PostgreSQL Connection Configuration ---\n",
    "print(\"\\n--- Configuring PostgreSQL Connection ---\")\n",
    "\n",
    "PG_HOST = \"postgres\"\n",
    "PG_PORT = \"5432\"\n",
    "PG_DATABASE = \"amazon_reviews_curated\"\n",
    "PG_USER = \"spark_writer\"\n",
    "PG_PASSWORD = \"password\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddb22b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "PG_URL = f\"jdbc:postgresql://{PG_HOST}:{PG_PORT}/{PG_DATABASE}\"\n",
    "PG_DRIVER = \"org.postgresql.Driver\"\n",
    "PG_PROPERTIES = {\n",
    "    \"user\": PG_USER,\n",
    "    \"password\": PG_PASSWORD,\n",
    "    \"driver\": PG_DRIVER,\n",
    "    \"stringtype\": \"unspecified\" \n",
    "}\n",
    "print(f\"PostgreSQL URL: {PG_URL}\")\n",
    "print(f\"PostgreSQL User: {PG_USER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hive_to_postgres(spark_session, hive_db, hive_table, pg_table, pg_url_conn, pg_props, save_mode=\"overwrite\"):\n",
    "    \"\"\"Reads from Hive table, handles specific type conversions, and writes to PostgreSQL.\"\"\"\n",
    "    full_hive_table = f\"{hive_db}.{hive_table}\"\n",
    "    print(f\"\\n--- [Loading {full_hive_table} to PG {pg_table}] Starting ---\")\n",
    "    start_load_time = time.time()\n",
    "    try:\n",
    "        # Read from Hive table\n",
    "        print(f\"Reading from Hive table: {full_hive_table}...\")\n",
    "        df_hive = spark_session.table(full_hive_table)\n",
    "        read_count = df_hive.count() \n",
    "        print(f\"Read {read_count} records from {full_hive_table}.\")\n",
    "\n",
    "        # Schema handling and transformations\n",
    "        df_to_write = df_hive\n",
    "\n",
    "        # Xử lí cho rating_distribution (MAP -> JSONB)\n",
    "        if 'rating_distribution' in df_to_write.columns:\n",
    "            \n",
    "            is_map = any(isinstance(field.dataType, MapType) for field in df_to_write.schema.fields if field.name == 'rating_distribution')\n",
    "            if is_map:\n",
    "                print(\"Converting 'rating_distribution' column from MAP to JSON string...\")\n",
    "                df_to_write = df_to_write.withColumn(\"rating_distribution\", F.to_json(F.col(\"rating_distribution\")))\n",
    "            else:\n",
    "                print(\"'rating_distribution' column is likely already String/JSON, skipping conversion.\")\n",
    "\n",
    "\n",
    "        if hive_table == \"product_overall_summary_materialized\" and \"first_letter_parent_asin\" in df_to_write.columns:\n",
    "            print(\"Dropping Hive partition column 'first_letter_parent_asin' before writing to PG.\")\n",
    "            df_to_write = df_to_write.drop(\"first_letter_parent_asin\")\n",
    "\n",
    "        # Write to PostgreSQL\n",
    "        print(f\"Writing {read_count} records to PostgreSQL table: {pg_table} (mode: {save_mode})...\")\n",
    "        start_write = time.time()\n",
    "        (df_to_write.write\n",
    "            .jdbc(url=pg_url_conn,\n",
    "                  table=pg_table,\n",
    "                  mode=save_mode,\n",
    "                  properties=pg_props)\n",
    "        )\n",
    "        write_duration = time.time() - start_write\n",
    "        print(f\"Successfully wrote to PostgreSQL table '{pg_table}' in {write_duration:.2f} seconds.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        print(f\"!!! ERROR loading {full_hive_table} to PG {pg_table}: {e}\")\n",
    "        print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "        traceback.print_exc()\n",
    "    finally:\n",
    "        load_duration = time.time() - start_load_time\n",
    "        print(f\"--- [Loading {full_hive_table} to PG {pg_table}] Finished in {load_duration:.2f} seconds ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a07bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_mappings = {\n",
    "    \"kindle_reviews_base\": \"kindle_reviews_base_pg\",\n",
    "    \"product_monthly_summary_materialized\": \"product_monthly_summary_pg\",\n",
    "    \"parent_product_overall_summary_materialized\": \"parent_product_overall_summary_pg\",\n",
    "    \"product_overall_summary_materialized\": \"product_overall_summary_pg\",\n",
    "    \"review_stats_monthly_materialized\": \"review_stats_monthly_pg\",\n",
    "    \"kindle_reviews_enriched_sample\": \"kindle_reviews_enriched_sample_pg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIVE_DB_SOURCE = \"curated\"\n",
    "\n",
    "print(\"\\n==========================================================\")\n",
    "print(\"=== Starting Data Loading from Hive Curated to PostgreSQL ===\")\n",
    "print(\"==========================================================\")\n",
    "overall_start_time = time.time()\n",
    "\n",
    "for hive_tbl, pg_tbl in table_mappings.items():\n",
    "    load_hive_to_postgres(spark, HIVE_DB_SOURCE, hive_tbl, pg_tbl, PG_URL, PG_PROPERTIES)\n",
    "\n",
    "overall_end_time = time.time()\n",
    "print(\"\\n==========================================================\")\n",
    "print(\"=== All Data Loading Jobs Completed ===\")\n",
    "print(f\"=== Total Execution Time: {overall_end_time - overall_start_time:.2f} seconds ({ (overall_end_time - overall_start_time)/60 :.2f} minutes) ===\")\n",
    "print(\"==========================================================\")\n",
    "\n",
    "\n",
    "print(\"\\nStopping Spark Session...\")\n",
    "stop_spark()\n",
    "print(\"Spark Session stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
