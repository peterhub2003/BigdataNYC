{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fff8d28-cd40-4b7b-bbc8-7b2513ea1ef2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/bitnami/spark\n",
      "/usr/local/spark\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('SPARK_HOME'))\n",
    "os.environ['SPARK_HOME'] = '/usr/local/spark'\n",
    "print(os.environ.get('SPARK_HOME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "329f6926-34fc-4c55-90ac-0b751579510c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting great_expectations\n",
      "  Downloading great_expectations-1.3.12-py3-none-any.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (2.8.2)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from great_expectations) (21.3)\n",
      "Requirement already satisfied: requests>=2.20 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.59.0 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (4.64.1)\n",
      "Requirement already satisfied: cryptography>=3.2 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (38.0.2)\n",
      "Collecting posthog<4,>3\n",
      "  Downloading posthog-3.23.0-py2.py3-none-any.whl (84 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tzlocal>=1.2\n",
      "  Downloading tzlocal-5.3.1-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing>=2.4 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (4.4.0)\n",
      "Collecting marshmallow<4.0.0,>=3.7.1\n",
      "  Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jsonschema>=2.5.1 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (4.16.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (1.23.4)\n",
      "Collecting altair<5.0.0,>=4.2.1\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m813.6/813.6 kB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic>=1.10.7\n",
      "  Downloading pydantic-2.11.1-py3-none-any.whl (442 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.6/442.6 kB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2>=3 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (3.1.2)\n",
      "Requirement already satisfied: ruamel.yaml>=0.16 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (0.17.21)\n",
      "Requirement already satisfied: mistune>=0.8.4 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (2.0.4)\n",
      "Requirement already satisfied: pandas<2.2,>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from great_expectations) (1.5.1)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.12.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.10/site-packages (from altair<5.0.0,>=4.2.1->great_expectations) (0.4)\n",
      "Requirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.10/site-packages (from cryptography>=3.2->great_expectations) (1.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3->great_expectations) (2.1.1)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.5.1->great_expectations) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=2.5.1->great_expectations) (22.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<2.2,>=1.3.0->great_expectations) (2022.5)\n",
      "Collecting backoff>=1.10.0\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting distro>=1.5.0\n",
      "  Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from posthog<4,>3->great_expectations) (1.16.0)\n",
      "Collecting annotated-types>=0.6.0\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Collecting pydantic-core==2.33.0\n",
      "  Downloading pydantic_core-2.33.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-extensions>=4.1.0\n",
      "  Downloading typing_extensions-4.13.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typing-inspection>=0.4.0\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->great_expectations) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->great_expectations) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->great_expectations) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20->great_expectations) (3.4)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.6 in /opt/conda/lib/python3.10/site-packages (from ruamel.yaml>=0.16->great_expectations) (0.2.6)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.2->great_expectations) (2.21)\n",
      "Installing collected packages: monotonic, tzlocal, typing-extensions, distro, backoff, annotated-types, typing-inspection, pydantic-core, posthog, marshmallow, pydantic, altair, great_expectations\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.4.0\n",
      "    Uninstalling typing_extensions-4.4.0:\n",
      "      Successfully uninstalled typing_extensions-4.4.0\n",
      "  Attempting uninstall: altair\n",
      "    Found existing installation: altair 4.2.0\n",
      "    Uninstalling altair-4.2.0:\n",
      "      Successfully uninstalled altair-4.2.0\n",
      "Successfully installed altair-4.2.2 annotated-types-0.7.0 backoff-2.2.1 distro-1.9.0 great_expectations-1.3.12 marshmallow-3.26.1 monotonic-1.6 posthog-3.23.0 pydantic-2.11.1 pydantic-core-2.33.0 typing-extensions-4.13.0 typing-inspection-0.4.0 tzlocal-5.3.1\n"
     ]
    }
   ],
   "source": [
    "!pip install great_expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c26d426f-df48-4163-a277-418401acb147",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped existing SparkContext\n",
      "Spark version: 3.3.0\n",
      "Spark UI: http://jupyter:4040\n"
     ]
    }
   ],
   "source": [
    "# First, stop any existing SparkContext\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sc.stop()\n",
    "    print(\"Stopped existing SparkContext\")\n",
    "except Exception as e:\n",
    "    print(f\"No existing SparkContext to stop or error occurred: {e}\")\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, explode, lit, array\n",
    "import time\n",
    "import os\n",
    "\n",
    "\n",
    "# Create a Spark session with explicit cluster configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Explicit Spark Job Test\") \\\n",
    "    .master(\"yarn\") \\\n",
    "    .config(\"spark.driver.host\", \"jupyter\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.yarn.am.memory\", \"1g\") \\\n",
    "    .config(\"spark.yarn.am.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.default.parallelism\", \"10\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.sql.files.maxPartitionBytes\", \"128m\") \\\n",
    "    .config(\"spark.sql.caseSensitive\", \"false\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "884d49bc-0523-4b10-8e86-04ac960270fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema Defined.\n",
      "Successfully started reading from: hdfs:///data/raw/amazon_reviews/Kindle_Store.jsonl\n",
      "root\n",
      " |-- rating: float (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- images: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- asin: string (nullable = true)\n",
      " |-- parent_asin: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      " |-- verified_purchase: boolean (nullable = true)\n",
      " |-- helpful_vote: integer (nullable = true)\n",
      "\n",
      "Raw data count: 25577616\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, FloatType,\n",
    "                               ArrayType, LongType, BooleanType, IntegerType,\n",
    "                               TimestampType)\n",
    "from pyspark.sql.functions import from_unixtime, year, month, col\n",
    "\n",
    "HDFS_RAW_FILE = \"hdfs:///data/raw/amazon_reviews/Kindle_Store.jsonl\"\n",
    "HDFS_PROCESSED_DIR = \"hdfs:///data/processed/amazon_reviews/kindle_store\" # Thư mục chứa Parquet partitions\n",
    "HIVE_DATABASE_NAME = \"amazon_data\" # Tên database Hive (tạo nếu chưa có)\n",
    "HIVE_TABLE_NAME = \"kindle_reviews_processed\"\n",
    "\n",
    "\n",
    "\n",
    "# --- 1. Define Schema based on the image ---\n",
    "# Lưu ý: timestamp là Unix epoch nên dùng LongType hoặc IntegerType\n",
    "# rating có thể là số nguyên hoặc thập phân, FloatType an toàn hơn\n",
    "# images là list các string (url hoặc id?)\n",
    "schema = StructType([\n",
    "    StructField(\"rating\", FloatType(), True),\n",
    "    StructField(\"title\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"images\", ArrayType(StringType()), True), # List of strings\n",
    "    StructField(\"asin\", StringType(), True), # Product ID\n",
    "    StructField(\"parent_asin\", StringType(), True), # Parent Product ID\n",
    "    StructField(\"user_id\", StringType(), True), # User ID\n",
    "    StructField(\"timestamp\", LongType(), True), # Unix timestamp (seconds)\n",
    "    StructField(\"verified_purchase\", BooleanType(), True),\n",
    "    StructField(\"helpful_vote\", IntegerType(), True)\n",
    "])\n",
    "print(\"Schema Defined.\")\n",
    "\n",
    "try:\n",
    "    # df_raw = spark.read.schema(schema).parquet(HDFS_PROCESSED_DIR)\n",
    "    # print(f\"Successfully started reading from: {HDFS_PROCESSED_DIR}\")\n",
    "    df_raw = spark.read.schema(schema).json(HDFS_RAW_FILE)\n",
    "    print(f\"Successfully started reading from: {HDFS_RAW_FILE}\")\n",
    "    df_raw.printSchema() # Kiểm tra schema sau khi đọc\n",
    "    print(f\"Raw data count: {df_raw.count()}\") # Đếm số dòng (có thể chậm với dữ liệu lớn)\n",
    "except Exception as e:\n",
    "    print(f\"Error reading raw data from {HDFS_RAW_FILE}: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4623b90-f24b-4b83-a832-db024b0b328b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= CLEANING ============\n",
      "\n",
      "+-------------+-------------------+----+-----+----------+\n",
      "|timestamp    |review_time        |year|month|date_str  |\n",
      "+-------------+-------------------+----+-----+----------+\n",
      "|1427541413000|2015-03-28 11:16:53|2015|3    |2015-03-28|\n",
      "|1504226946142|2017-09-01 00:49:06|2017|9    |2017-09-01|\n",
      "|1644883955777|2022-02-15 00:12:35|2022|2    |2022-02-15|\n",
      "|1363027885000|2013-03-11 18:51:25|2013|3    |2013-03-11|\n",
      "|1637557512064|2021-11-22 05:05:12|2021|11   |2021-11-22|\n",
      "|1637134078567|2021-11-17 07:27:58|2021|11   |2021-11-17|\n",
      "|1632291278732|2021-09-22 06:14:38|2021|9    |2021-09-22|\n",
      "|1614145710980|2021-02-24 05:48:30|2021|2    |2021-02-24|\n",
      "|1599452688091|2020-09-07 04:24:48|2020|9    |2020-09-07|\n",
      "|1574812541555|2019-11-26 23:55:41|2019|11   |2019-11-26|\n",
      "|1568214752013|2019-09-11 15:12:32|2019|9    |2019-09-11|\n",
      "|1567293346345|2019-08-31 23:15:46|2019|8    |2019-08-31|\n",
      "|1566774264228|2019-08-25 23:04:24|2019|8    |2019-08-25|\n",
      "|1558889036351|2019-05-26 16:43:56|2019|5    |2019-05-26|\n",
      "|1558333337162|2019-05-20 06:22:17|2019|5    |2019-05-20|\n",
      "|1546122375646|2018-12-29 22:26:15|2018|12   |2018-12-29|\n",
      "|1543044737414|2018-11-24 07:32:17|2018|11   |2018-11-24|\n",
      "|1542164901708|2018-11-14 03:08:21|2018|11   |2018-11-14|\n",
      "|1537226264482|2018-09-17 23:17:44|2018|9    |2018-09-17|\n",
      "|1537226257534|2018-09-17 23:17:37|2018|9    |2018-09-17|\n",
      "+-------------+-------------------+----+-----+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_unixtime, year, month, dayofmonth, date_format, coalesce, lit, trim, when\n",
    "\n",
    "print(\"============= CLEANING ============\\n\")\n",
    "df_cleaned = df_raw \\\n",
    "    .filter(col(\"text\").isNotNull() & (trim(col(\"text\")) != \"\")) \\\n",
    "    .filter(col(\"rating\").isNotNull()) \\\n",
    "    .filter(col(\"asin\").isNotNull() & (trim(col(\"asin\")) != \"\")) \\\n",
    "    .filter(col(\"user_id\").isNotNull() & (trim(col(\"user_id\")) != \"\")) \\\n",
    "    .withColumn(\"title\", trim(coalesce(col(\"title\"), lit(\"[no title]\")))) \\\n",
    "    .withColumn(\"text\", trim(col(\"text\"))) \\\n",
    "    .withColumn(\"asin\", trim(col(\"asin\"))) \\\n",
    "    .withColumn(\"parent_asin\", trim(col(\"parent_asin\"))) \\\n",
    "    .withColumn(\"user_id\", trim(col(\"user_id\"))) \\\n",
    "    .withColumn(\"helpful_vote\", coalesce(col(\"helpful_vote\"), lit(0)).cast(IntegerType())) \\\n",
    "    .withColumn(\"images\", coalesce(col(\"images\"), array().cast(ArrayType(StringType())))) \\\n",
    "    .withColumn(\"review_time\", from_unixtime(col(\"timestamp\") / 1000).cast(TimestampType())) \\\n",
    "    .filter(col(\"review_time\").isNotNull()) # Loại bỏ nếu timestamp không hợp lệ\n",
    "\n",
    "df_cleaned = df_cleaned.withColumn(\"year\", year(col(\"review_time\"))) \\\n",
    "                     .withColumn(\"month\", month(col(\"review_time\"))) \\\n",
    "                     .withColumn(\"day\", dayofmonth(col(\"review_time\"))) \\\n",
    "                     .withColumn(\"date_str\", date_format(col(\"review_time\"), \"yyyy-MM-dd\")) \\\n",
    "                     .filter(col(\"year\").isNotNull() & (col(\"year\") >= 1990) & (col(\"year\") <= 2025))\n",
    "\n",
    "df_cleaned.select(\"timestamp\", \"review_time\", \"year\", \"month\", \"date_str\").show(20, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "82dadb40-fa40-4de1-a673-7764775ebaa9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'deprecated' from 'typing_extensions' (/opt/conda/lib/python3.10/site-packages/typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgx\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataContextError\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofile\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01muser_configurable_profiler\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UserConfigurableProfiler\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/__init__.py:10\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m _get_versions\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# great_expectations.data_context must be imported first or we will have circular dependency issues\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m  \u001b[38;5;66;03m# isort:skip\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_context\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/data_context/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     AbstractDataContext,\n\u001b[1;32m      3\u001b[0m     CloudDataContext,\n\u001b[1;32m      4\u001b[0m     EphemeralDataContext,\n\u001b[1;32m      5\u001b[0m     FileDataContext,\n\u001b[1;32m      6\u001b[0m     get_context,\n\u001b[1;32m      7\u001b[0m     project_manager,\n\u001b[1;32m      8\u001b[0m     set_context,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/data_context/data_context/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mabstract_data_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      2\u001b[0m     AbstractDataContext,\n\u001b[1;32m      3\u001b[0m )\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloud_data_context\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     CloudDataContext,\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext_factory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     get_context,\n\u001b[1;32m      9\u001b[0m     project_manager,\n\u001b[1;32m     10\u001b[0m     set_context,\n\u001b[1;32m     11\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/data_context/data_context/abstract_data_context.py:38\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgx_exceptions\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docs_decorators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     33\u001b[0m     deprecated_method_or_class,\n\u001b[1;32m     34\u001b[0m     new_argument,\n\u001b[1;32m     35\u001b[0m     new_method_or_class,\n\u001b[1;32m     36\u001b[0m     public_api,\n\u001b[1;32m     37\u001b[0m )\n\u001b[0;32m---> 38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init \u001b[38;5;28;01mas\u001b[39;00m init_analytics\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m submit \u001b[38;5;28;01mas\u001b[39;00m submit_event\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ENV_CONFIG\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/analytics/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init, submit\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/analytics/client.py:9\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UUID\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mposthog\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ENV_CONFIG, Config, update_config\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01manalytics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_event\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Event\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/analytics/config.py:6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal, Optional\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01muuid\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UUID\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompatibility\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpydantic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BaseSettings,\n\u001b[1;32m      8\u001b[0m     GenericModel,\n\u001b[1;32m      9\u001b[0m     HttpUrl,\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_EnvConfig\u001b[39;00m(BaseSettings):\n\u001b[1;32m     14\u001b[0m     gx_analytics_enabled: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/great_expectations/compatibility/pydantic.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydantic\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompatibility\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnot_imported\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      6\u001b[0m     is_version_greater_or_equal,\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_version_greater_or_equal(version\u001b[38;5;241m=\u001b[39mpydantic\u001b[38;5;241m.\u001b[39mVERSION, compare_version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# TODO: don't use star imports\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m import_module\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_migration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getattr_migration\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VERSION\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typing\u001b[38;5;241m.\u001b[39mTYPE_CHECKING:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# import of virtually everything is supported via `__getattr__` below,\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# but we need them here for type checking and IDE support\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic/_migration.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Callable\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m version_short\n\u001b[1;32m      6\u001b[0m MOVED_IN_V2 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.utils:version_info\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.version:version_info\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.error_wrappers:ValidationError\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic:ValidationError\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.generics:GenericModel\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.BaseModel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m }\n\u001b[1;32m     16\u001b[0m DEPRECATED_MOVED_IN_V2 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.tools:schema_of\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.deprecated.tools:schema_of\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.tools:parse_obj_as\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.deprecated.tools:parse_obj_as\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.config:Extra\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpydantic.deprecated.config:Extra\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     29\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic/version.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"The `version` module holds the version information for Pydantic.\"\"\"\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m annotations \u001b[38;5;28;01mas\u001b[39;00m _annotations\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpydantic_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__ \u001b[38;5;28;01mas\u001b[39;00m __pydantic_core_version__\n\u001b[1;32m      7\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVERSION\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mversion_info\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      9\u001b[0m VERSION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2.11.1\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic_core/__init__.py:30\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any \u001b[38;5;28;01mas\u001b[39;00m _Any\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pydantic_core\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     ArgsKwargs,\n\u001b[1;32m      8\u001b[0m     MultiHostUrl,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     validate_core_schema,\n\u001b[1;32m     29\u001b[0m )\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore_schema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CoreConfig, CoreSchema, CoreSchemaType, ErrorType\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m11\u001b[39m):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NotRequired \u001b[38;5;28;01mas\u001b[39;00m _NotRequired\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pydantic_core/core_schema.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pattern\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Literal, Union\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m12\u001b[39m):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TypedDict\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'deprecated' from 'typing_extensions' (/opt/conda/lib/python3.10/site-packages/typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "import great_expectations as gx\n",
    "from great_expectations.exceptions import DataContextError\n",
    "from great_expectations.profile.user_configurable_profiler import UserConfigurableProfiler\n",
    "from pyspark.sql import SparkSession # Chỉ để minh họa, bạn đã có session 'spark'\n",
    "\n",
    "\n",
    "print(\"--- Starting Great Expectations Profiling ---\")\n",
    "\n",
    "# --- 1. Initialize Great Expectations Data Context ---\n",
    "# Data Context quản lý cấu hình, datasources, suites, checkpoints...\n",
    "# Mặc định, nó sẽ tạo cấu trúc thư mục 'great_expectations' trong thư mục làm việc hiện tại\n",
    "try:\n",
    "    context = gx.get_context()\n",
    "    print(\"Existing Great Expectations context loaded.\")\n",
    "except DataContextError:\n",
    "    context = gx.DataContext.create()\n",
    "    print(\"New Great Expectations context created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error getting or creating Great Expectations context: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "\n",
    "# --- 2. Add Spark DataFrame as a Datasource and Data Asset ---\n",
    "# Datasource định nghĩa cách GX kết nối với dữ liệu (ở đây là Spark)\n",
    "# Data Asset đại diện cho DataFrame cụ thể của chúng ta\n",
    "datasource_name = \"my_spark_datasource\" # Đặt tên tùy ý\n",
    "asset_name = \"kindle_reviews_cleaned_asset\" # Đặt tên tùy ý\n",
    "\n",
    "try:\n",
    "    # Thêm Spark Datasource nếu chưa tồn tại\n",
    "    datasource = context.sources.add_spark(name=datasource_name)\n",
    "    print(f\"Spark Datasource '{datasource_name}' added.\")\n",
    "except DataContextError:\n",
    "    datasource = context.get_datasource(datasource_name)\n",
    "    print(f\"Spark Datasource '{datasource_name}' already exists, using it.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding or getting Spark Datasource: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "try:\n",
    "    # Thêm DataFrame làm Data Asset vào Datasource\n",
    "    # Quan trọng: Truyền DataFrame 'df_cleaned' vào đây\n",
    "    data_asset = datasource.add_dataframe_asset(name=asset_name, dataframe=df_cleaned)\n",
    "    print(f\"DataFrame Asset '{asset_name}' added to Datasource '{datasource_name}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding DataFrame Asset: {e}\")\n",
    "    # Có thể cần xóa asset cũ nếu chạy lại: context.delete_asset(asset_name=f\"{datasource_name}/{asset_name}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# --- 3. Create or Get an Expectation Suite ---\n",
    "# Expectation Suite là nơi lưu trữ các kỳ vọng (Expectations) về dữ liệu\n",
    "# Profiler sẽ tự động điền vào suite này\n",
    "expectation_suite_name = \"kindle_reviews_cleaned_profiling_suite\" # Đặt tên tùy ý\n",
    "\n",
    "try:\n",
    "    suite = context.add_expectation_suite(expectation_suite_name=expectation_suite_name)\n",
    "    print(f\"New Expectation Suite '{expectation_suite_name}' created.\")\n",
    "except DataContextError:\n",
    "    suite = context.get_expectation_suite(expectation_suite_name=expectation_suite_name)\n",
    "    print(f\"Expectation Suite '{expectation_suite_name}' already exists, using it.\")\n",
    "    # Có thể bạn muốn xóa các expectations cũ trước khi profiling lại\n",
    "    # suite.expectations = []\n",
    "except Exception as e:\n",
    "    print(f\"Error adding or getting Expectation Suite: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# --- 4. Run the UserConfigurableProfiler ---\n",
    "# Profiler này sẽ quét dữ liệu trong Data Asset và tự động tạo ra các Expectations\n",
    "# Quá trình này thực chất là thực hiện các tính toán profiling trên Spark\n",
    "print(f\"Running UserConfigurableProfiler on asset '{asset_name}'...\")\n",
    "profiler = UserConfigurableProfiler(profile_dataset=data_asset)\n",
    "suite = profiler.build_suite() # Đây là lúc profiling thực sự diễn ra\n",
    "print(\"Profiler finished building the suite based on data characteristics.\")\n",
    "\n",
    "# Lưu Expectation Suite đã được điền bởi Profiler\n",
    "try:\n",
    "    context.save_expectation_suite(expectation_suite=suite)\n",
    "    print(f\"Expectation Suite '{expectation_suite_name}' saved with profiled expectations.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving Expectation Suite: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# --- 5. Configure and Run a Checkpoint ---\n",
    "# Checkpoint định nghĩa cách chạy validation (và profiling đi kèm)\n",
    "# Nó sẽ chạy Expectation Suite trên Data Asset và tạo ra kết quả validation\n",
    "checkpoint_name = \"kindle_reviews_cleaned_profiling_checkpoint\" # Đặt tên tùy ý\n",
    "\n",
    "# Cấu hình Checkpoint đơn giản\n",
    "checkpoint_config = {\n",
    "    \"name\": checkpoint_name,\n",
    "    \"config_version\": 1.0,\n",
    "    \"class_name\": \"SimpleCheckpoint\", # Lớp Checkpoint đơn giản\n",
    "    \"run_name_template\": \"%Y%m%d-%H%M%S-profile-run\", # Mẫu tên cho mỗi lần chạy\n",
    "    \"validations\": [ # Danh sách các validation cần chạy\n",
    "        {\n",
    "            \"batch_request\": data_asset.build_batch_request(), # Yêu cầu dữ liệu từ Data Asset\n",
    "            \"expectation_suite_name\": expectation_suite_name, # Sử dụng suite vừa được profiler tạo\n",
    "        }\n",
    "    ],\n",
    "    # Action để tự động cập nhật và mở Data Docs sau khi chạy\n",
    "     \"action_list\": [\n",
    "        {\n",
    "            \"name\": \"store_validation_result\",\n",
    "            \"action\": {\"class_name\": \"StoreValidationResultAction\"},\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"update_data_docs\",\n",
    "            \"action\": {\"class_name\": \"UpdateDataDocsAction\", \"site_names\": []},\n",
    "        },\n",
    "        # { # Bỏ comment dòng này nếu muốn tự mở Data Docs trong trình duyệt\n",
    "        #     \"name\": \"open_data_docs\",\n",
    "        #     \"action\": {\"class_name\": \"OpenDataDocsAction\"}\n",
    "        # }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Thêm hoặc cập nhật Checkpoint vào context\n",
    "try:\n",
    "    context.add_or_update_checkpoint(**checkpoint_config)\n",
    "    print(f\"Checkpoint '{checkpoint_name}' added or updated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error adding or updating Checkpoint: {e}\")\n",
    "    spark.stop()\n",
    "    exit(1)\n",
    "\n",
    "# Chạy Checkpoint\n",
    "print(f\"Running Checkpoint '{checkpoint_name}'...\")\n",
    "results = context.run_checkpoint(checkpoint_name=checkpoint_name)\n",
    "print(\"Checkpoint run finished.\")\n",
    "\n",
    "if not results[\"success\"]:\n",
    "    print(\"Checkpoint run failed or had validation errors.\")\n",
    "    # Nên kiểm tra chi tiết lỗi trong 'results' object hoặc Data Docs\n",
    "else:\n",
    "    print(\"Checkpoint run succeeded.\")\n",
    "\n",
    "# --- 6. Review Profiling Results in Data Docs ---\n",
    "# Cách tốt nhất để xem kết quả profiling là qua Data Docs (báo cáo HTML)\n",
    "print(\"\\n--- Review Results ---\")\n",
    "print(\"Building Data Docs...\")\n",
    "try:\n",
    "    context.build_data_docs()\n",
    "    print(\"Data Docs build complete.\")\n",
    "    print(f\"To view the profiling results, open the Data Docs HTML file, usually located at: great_expectations/uncommitted/data_docs/local_site/index.html\")\n",
    "    # Hoặc nếu bạn muốn mở tự động (cần bỏ comment action ở trên):\n",
    "    # context.open_data_docs()\n",
    "except Exception as e:\n",
    "    print(f\"Error building or opening Data Docs: {e}\")\n",
    "\n",
    "# (Optional) Truy cập metrics programmatically từ results object\n",
    "# Việc này phức tạp hơn xem Data Docs\n",
    "try:\n",
    "    validation_result_identifier = results.list_validation_result_identifiers()[0]\n",
    "    validation_result = results.get_validation_result(identifier=validation_result_identifier)\n",
    "    # 'validation_result.results' là list các kết quả của từng expectation\n",
    "    # Mỗi kết quả chứa 'observed_value' là metric đã tính toán\n",
    "    print(f\"\\nExample metrics from results object (column: rating):\")\n",
    "    for evr in validation_result.results:\n",
    "        if evr.expectation_config.kwargs.get(\"column\") == \"rating\":\n",
    "             metric_name = evr.expectation_config.expectation_type\n",
    "             observed_value = evr.result.get(\"observed_value\")\n",
    "             print(f\"- Expectation/Metric: {metric_name}, Observed Value: {observed_value}\")\n",
    "except Exception as e:\n",
    "     print(f\"Could not extract programmatic metrics: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Great Expectations Profiling Finished ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ec5bb2b-47c6-45eb-8ba8-34f8c6877e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing validation failures to: hdfs:///data/processed/amazon_reviews/kindle_store_validation_failures\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "\n",
    "print(\"=========  VALIDATION ==========\\n\")\n",
    "# Tạo cột lý do lỗi (ban đầu là null)\n",
    "df_validated = df_cleaned.withColumn(\"validation_error\", lit(None).cast(StringType()))\n",
    "\n",
    "# Áp dụng các quy tắc validation\n",
    "df_validated = df_validated.withColumn(\"validation_error\",\n",
    "    when((col(\"rating\") < 1.0) | (col(\"rating\") > 5.0), \"Invalid Rating\")\n",
    "    .otherwise(col(\"validation_error\"))) # Giữ lỗi cũ nếu có\n",
    "\n",
    "df_validated = df_validated.withColumn(\"validation_error\",\n",
    "    when(col(\"helpful_vote\") < 0, \"Negative Helpful Vote\")\n",
    "    .otherwise(col(\"validation_error\")))\n",
    "\n",
    "# Ví dụ kiểm tra độ dài ASIN\n",
    "df_validated = df_validated.withColumn(\"validation_error\",\n",
    "    when(length(col(\"asin\")) != 10, \"Invalid ASIN Length\")\n",
    "    .otherwise(col(\"validation_error\")))\n",
    "\n",
    "# Kiểm tra timestamp hợp lý (ví dụ: không sau ngày hôm nay)\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "df_validated = df_validated.withColumn(\"validation_error\",\n",
    "    when(col(\"review_time\") > current_timestamp(), \"Future Timestamp\")\n",
    "    .otherwise(col(\"validation_error\")))\n",
    "\n",
    "# Tách thành 2 DataFrame: Hợp lệ và Không hợp lệ\n",
    "df_valid = df_validated.filter(col(\"validation_error\").isNull()).drop(\"validation_error\")\n",
    "df_invalid = df_validated.filter(col(\"validation_error\").isNotNull())\n",
    "\n",
    "\n",
    "# Lưu các bản ghi không hợp lệ để phân tích (Bảng Silver phụ)\n",
    "HDFS_VALIDATION_FAILURES_DIR = \"hdfs:///data/processed/amazon_reviews/kindle_store_validation_failures\"\n",
    "print(f\"Writing validation failures to: {HDFS_VALIDATION_FAILURES_DIR}\")\n",
    "df_invalid.write \\\n",
    "    .partitionBy(\"year\", \"month\", \"validation_error\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(HDFS_VALIDATION_FAILURES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8ad723ec-3073-47f2-81f4-6c811522eeba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============== STANRDALIZATION ===========\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25576202"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower\n",
    "\n",
    "print(\"============== STANRDALIZATION ===========\\n\")\n",
    "df_standardized = df_valid \\\n",
    "    .withColumn(\"title_processed\", lower(col(\"title\"))) \\\n",
    "    .withColumn(\"text_processed\", lower(col(\"text\")))\n",
    "    # Giữ lại cột gốc nếu cần so sánh\n",
    "    \n",
    "df_standardized.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eb765671-0894-4f13-8f6c-2c929e69e74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplication complete. Count after deduplication: 25300905\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"user_id\", \"asin\").orderBy(col(\"review_time\").desc())\n",
    "\n",
    "df_deduplicated = df_standardized.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "                               .filter(col(\"rn\") == 1) \\\n",
    "                               .drop(\"rn\")\n",
    "    \n",
    "print(f\"Deduplication complete. Count after deduplication: {df_deduplicated.count()}\") # Cẩn thận khi count() trên data lớn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fec98202-8c82-4194-bf90-83239693c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_processed = df_deduplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e4885f01-0c5e-470d-a0b2-207713eac9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "HDFS_PROCESSED_BASE = \"hdfs:///data/processed/amazon_reviews\"\n",
    "\n",
    "# Hive Database Names\n",
    "HIVE_PROCESSED_DB = \"processed\"\n",
    "\n",
    "HDFS_PROCESSED_MAIN_DIR = f\"{HDFS_PROCESSED_BASE}/kindle_store_main\"\n",
    "HIVE_PROCESSED_MAIN_TABLE = \"kindle_reviews_main\"\n",
    "\n",
    "# 2. Validation Failures Table (vẫn giữ nguyên)\n",
    "HDFS_VALIDATION_FAILURES_DIR = f\"{HDFS_PROCESSED_BASE}/kindle_store_validation_failures\"\n",
    "HIVE_VALIDATION_FAILURES_TABLE = \"kindle_reviews_validation_failures\"\n",
    "\n",
    "# 3. Product Dimension VIEW (sẽ tạo VIEW trên Hive)\n",
    "HIVE_PRODUCTS_DIM_VIEW = \"kindle_products_dim\"\n",
    "\n",
    "# 4. User Dimension VIEW (sẽ tạo VIEW trên Hive)\n",
    "HIVE_USERS_DIM_VIEW = \"kindle_users_dim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c571a224-0f1a-4c64-be17-f95de433f9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main processed data to: hdfs:///data/processed/amazon_reviews/kindle_store_main\n"
     ]
    }
   ],
   "source": [
    "df_to_save = df_final_processed.select(\n",
    "    \"rating\", \"title_processed\", \"text_processed\", \"images\", \"asin\",\n",
    "    \"parent_asin\", \"user_id\", \"verified_purchase\", \"helpful_vote\",\n",
    "    \"review_time\", \"year\", \"month\", \"day\", \"date_str\"\n",
    "    # Thêm cột gốc title, text nếu muốn: \"title\", \"text\",\n",
    ")\n",
    "\n",
    "print(f\"Writing main processed data to: {HDFS_PROCESSED_MAIN_DIR}\")\n",
    "df_to_save.write \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(HDFS_PROCESSED_MAIN_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "88751490-0142-4239-9557-bda46da2e339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Creating/Updating Hive Tables and VIEWS ---\n",
      "Ensured Hive database 'processed' exists.\n",
      "Creating Hive TABLE: processed.kindle_reviews_main\n",
      "Running MSCK REPAIR TABLE for kindle_reviews_main...\n",
      "Successfully created and repaired main processed table.\n",
      "Creating Hive TABLE: processed.kindle_reviews_validation_failures\n",
      "Running MSCK REPAIR TABLE for kindle_reviews_validation_failures...\n",
      "Successfully created and repaired validation failures table.\n",
      "Creating Hive VIEW: processed.kindle_products_dim\n",
      "Successfully created products dimension VIEW.\n",
      "Creating Hive VIEW: processed.kindle_users_dim\n",
      "Successfully created users dimension VIEW.\n",
      "\n",
      "--- Finished Simplified Storage and Hive Table/View Creation (Phase 3) ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Creating/Updating Hive Tables and VIEWS ---\")\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {HIVE_PROCESSED_DB}\")\n",
    "print(f\"Ensured Hive database '{HIVE_PROCESSED_DB}' exists.\")\n",
    "\n",
    "# Create Hive External TABLE for Main Processed Data\n",
    "print(f\"Creating Hive TABLE: {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE}\")\n",
    "create_main_table_sql = f\"\"\"\n",
    "CREATE EXTERNAL TABLE {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE} (\n",
    "    rating FLOAT,\n",
    "    title_processed STRING,\n",
    "    text_processed STRING,\n",
    "    images ARRAY<STRING>,\n",
    "    asin STRING,\n",
    "    parent_asin STRING,\n",
    "    user_id STRING,\n",
    "    verified_purchase BOOLEAN,\n",
    "    helpful_vote INT,\n",
    "    review_time TIMESTAMP,\n",
    "    date_str STRING\n",
    ")\n",
    "PARTITIONED BY (year INT, month INT)\n",
    "STORED AS PARQUET\n",
    "LOCATION '{HDFS_PROCESSED_MAIN_DIR}'\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "try:\n",
    "    spark.sql(create_main_table_sql)\n",
    "    print(f\"Running MSCK REPAIR TABLE for {HIVE_PROCESSED_MAIN_TABLE}...\")\n",
    "    spark.sql(f\"MSCK REPAIR TABLE {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE}\")\n",
    "    print(f\"Successfully created and repaired main processed table.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR creating/repairing main processed table: {e}\")\n",
    "\n",
    "\n",
    "# Create Hive External TABLE for Validation Failures (vẫn giữ nguyên)\n",
    "print(f\"Creating Hive TABLE: {HIVE_PROCESSED_DB}.{HIVE_VALIDATION_FAILURES_TABLE}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {HIVE_PROCESSED_DB}.{HIVE_VALIDATION_FAILURES_TABLE}\")\n",
    "create_failures_table_sql = f\"\"\"\n",
    "CREATE EXTERNAL TABLE {HIVE_PROCESSED_DB}.{HIVE_VALIDATION_FAILURES_TABLE} (\n",
    "    rating FLOAT,\n",
    "    title STRING,\n",
    "    text STRING,\n",
    "    asin STRING,\n",
    "    parent_asin STRING,\n",
    "    user_id STRING,\n",
    "    verified_purchase BOOLEAN,\n",
    "    helpful_vote INT,\n",
    "    timestamp LONG,\n",
    "    review_time TIMESTAMP,\n",
    "    date_str STRING\n",
    ")\n",
    "PARTITIONED BY (year INT, month INT, validation_error STRING)\n",
    "STORED AS PARQUET\n",
    "LOCATION '{HDFS_VALIDATION_FAILURES_DIR}'\n",
    "TBLPROPERTIES ('parquet.compression'='SNAPPY')\n",
    "\"\"\"\n",
    "try:\n",
    "    spark.sql(create_failures_table_sql)\n",
    "    print(f\"Running MSCK REPAIR TABLE for {HIVE_VALIDATION_FAILURES_TABLE}...\")\n",
    "    spark.sql(f\"MSCK REPAIR TABLE {HIVE_PROCESSED_DB}.{HIVE_VALIDATION_FAILURES_TABLE}\")\n",
    "    print(f\"Successfully created and repaired validation failures table.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR creating/repairing validation failures table: {e}\")\n",
    "\n",
    "\n",
    "# Create Hive VIEW for Products Dimension\n",
    "print(f\"Creating Hive VIEW: {HIVE_PROCESSED_DB}.{HIVE_PRODUCTS_DIM_VIEW}\")\n",
    "spark.sql(f\"DROP VIEW IF EXISTS {HIVE_PROCESSED_DB}.{HIVE_PRODUCTS_DIM_VIEW}\")\n",
    "create_products_view_sql = f\"\"\"\n",
    "CREATE VIEW {HIVE_PROCESSED_DB}.{HIVE_PRODUCTS_DIM_VIEW} AS\n",
    "SELECT\n",
    "    asin,\n",
    "    parent_asin,\n",
    "    MIN(review_time) AS first_review_time,\n",
    "    MAX(review_time) AS last_review_time,\n",
    "    AVG(rating) AS avg_rating,\n",
    "    COUNT(*) AS total_reviews,\n",
    "    SUM(helpful_vote) AS total_helpful_votes_received\n",
    "FROM {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE}\n",
    "GROUP BY asin, parent_asin\n",
    "\"\"\"\n",
    "try:\n",
    "    spark.sql(create_products_view_sql)\n",
    "    print(f\"Successfully created products dimension VIEW.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR creating products dimension VIEW: {e}\")\n",
    "\n",
    "\n",
    "# Create Hive VIEW for Users Dimension\n",
    "print(f\"Creating Hive VIEW: {HIVE_PROCESSED_DB}.{HIVE_USERS_DIM_VIEW}\")\n",
    "spark.sql(f\"DROP VIEW IF EXISTS {HIVE_PROCESSED_DB}.{HIVE_USERS_DIM_VIEW}\")\n",
    "create_users_view_sql = f\"\"\"\n",
    "CREATE VIEW {HIVE_PROCESSED_DB}.{HIVE_USERS_DIM_VIEW} AS\n",
    "SELECT\n",
    "    user_id,\n",
    "    MIN(review_time) AS first_review_time,\n",
    "    MAX(review_time) AS last_review_time,\n",
    "    COUNT(*) AS total_reviews_written,\n",
    "    AVG(rating) AS avg_rating_given,\n",
    "    SUM(helpful_vote) AS total_helpful_votes_on_written_reviews\n",
    "FROM {HIVE_PROCESSED_DB}.{HIVE_PROCESSED_MAIN_TABLE}\n",
    "GROUP BY user_id\n",
    "\"\"\"\n",
    "try:\n",
    "    spark.sql(create_users_view_sql)\n",
    "    print(f\"Successfully created users dimension VIEW.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR creating users dimension VIEW: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Finished Simplified Storage and Hive Table/View Creation (Phase 3) ---\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d192657-9d3c-433a-91b8-528fece3e562",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2d8641-7523-4305-a824-3dbe0e31fe9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
