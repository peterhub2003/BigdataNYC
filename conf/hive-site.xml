<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<configuration>
    <!-- PostgreSQL Metastore Configuration -->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:postgresql://postgres:5432/metastore</value>
        <description>JDBC connect string for a JDBC metastore</description>
    </property>
    
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>org.postgresql.Driver</value>
        <description>Driver class name for a JDBC metastore</description>
    </property>
    
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>ThangData</value>
        <description>Username to use against metastore database</description>
    </property>
    
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>password</value>
        <description>Password to use against metastore database</description>
    </property>
    
    <property>
        <name>hive.metastore.db.type</name>
        <value>postgres</value>
        <description>Type of database used by metastore</description>
    </property>
    
    <property>
        <name>javax.jdo.option.Multithreaded</name>
        <value>true</value>
    </property>
    
    <!-- Warehouse and HDFS Configuration -->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>hdfs://namenode:9000/user/hive/warehouse</value>
        <description>Location of default database for the warehouse</description>
    </property>
    
    <property>
        <name>hive.metastore.warehouse.external.dir</name>
        <value>hdfs://namenode:9000/user/hive/warehouse/external</value>
        <description>Location of external database for the warehouse</description>
    </property>
    
    <!-- Server Configuration -->
    <property>
        <name>hive.server2.enable.doAs</name>
        <value>false</value>
        <description>Disable user impersonation for Hive Server 2</description>
    </property>
    <property>
        <name>hive.server2.thrift.port</name>
        <value>10000</value>
    </property>
    <property>
        <name>hive.metastore.uris</name>
        <value>thrift://hive-metastore:9083</value>
        <description>Thrift URI for the remote metastore</description>
    </property>
    
    <!-- Performance Tuning -->
    <property>
        <name>hive.metastore.client.socket.timeout</name>
        <value>300</value>
        <description>MetaStore Client Socket timeout in seconds</description>
    </property>
    
    <property>
        <name>hive.metastore.event.db.notification.api.auth</name>
        <value>false</value>
        <description>Disable authorization for metastore notifications</description>
    </property>
    

    <property>
        <name>spark.yarn.appMasterEnv.PYSPARK_PYTHON</name>
        <value>/usr/bin/python3</value>
        <description>Python executable for PySpark in YARN AppMaster</description>
    </property>
    
    <property>
        <name>spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON</name>
        <value>/usr/bin/python3</value>
        <description>Python executable for PySpark driver</description>
    </property>
    
    <property>
        <name>spark.executorEnv.PYSPARK_PYTHON</name>
        <value>/usr/bin/python3</value>
        <description>Python executable for PySpark executors</description>
    </property>


    <property>
        <name>hive.execution.engine</name>
        <value>mr</value>
        <description>Chooses execution engine. Options are: mr, tez, spark. Use mr.</description>
    </property>

    <property>
        <name>spark.master</name>
        <value>yarn</value>
        <description>Spark master. Use yarn for YARN cluster.</description>
    </property>
    <property>
        <name>spark.submit.deployMode</name>
        <value>cluster</value> 
        <description>Deployment mode for Spark driver (cluster or client)</description>
    </property>
    <!-- <property>
        <name>spark.yarn.jars</name>
        <value>hdfs://namenode:9000/spark-jars/*</value>
        </description>
    </property> -->

    <property>
        <name>spark.executor.memory</name>
        <value>4g</value> 
        <description>Amount of memory to use per executor process</description>
    </property>

    <property>
        <name>spark.executor.cores</name>
        <value>2</value> 
        <description>Number of cores to use for each executor</description>
    </property>

    <property>
        <name>spark.driver.memory</name>
        <value>2g</value> 
        <description>Amount of memory to use for the driver process</description>
    </property>

    <property>
        <name>spark.executor.instances</name>
        <value>3</value> 
        <description>Initial number of executors to run</description>
    </property>

    <property>
        <name>spark.eventLog.enabled</name>
        <value>true</value>
        <description>Whether to log Spark events, useful for monitoring</description>
    </property>
    <property>
        <name>spark.eventLog.dir</name>
        <value>hdfs://namenode:9000/user/hadoop/spark-logs</value> 
        <description>Directory where Spark events are logged</description>
    </property>
    <property>
        <name>spark.history.fs.logDirectory</name>
        <value>hdfs://namenode:9000/user/hadoop/spark-logs</value>
        <description>Points Spark History Server to the log directory</description>
    </property>

</configuration>
