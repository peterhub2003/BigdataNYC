# Default system properties included when running spark.
# This is useful for setting default environmental settings.

# Set master to YARN
spark.master yarn

# HDFS Integration
spark.hadoop.fs.defaultFS hdfs://namenode:9000
spark.hadoop.dfs.replication 2


spark.submit.deployMode client

spark.yarn.am.memory 1g
spark.yarn.am.cores 1

# Executor settings
spark.executor.memory 2g
spark.executor.cores 2
spark.executor.instances 3
spark.yarn.executor.memoryOverhead 512m
spark.driver.memory 1g

# Overflow memory cho Python/PySpark
spark.yarn.executor.memoryOverhead 384m
spark.driver.memoryOverhead 384m


spark.yarn.jars hdfs://namenode:9000/user/hadoop/spark-jars/*

# Compression settings      
spark.io.compression.codec snappy
spark.sql.parquet.compression.codec snappy
spark.hadoop.io.compression.codecs org.apache.hadoop.io.compress.DefaultCodec,org.apache.hadoop.io.compress.GzipCodec,org.apache.hadoop.io.compress.SnappyCodec


# Dynamic allocation
spark.dynamicAllocation.enabled true
spark.dynamicAllocation.initialExecutors 1
spark.dynamicAllocation.minExecutors 1
spark.dynamicAllocation.maxExecutors 3
spark.dynamicAllocation.executorIdleTimeout 60s



# History server
spark.eventLog.enabled true
spark.eventLog.dir hdfs://namenode:9000/user/hadoop/spark-logs
spark.history.fs.logDirectory hdfs://namenode:9000/user/hadoop/spark-logs

# Serialization
spark.serializer org.apache.spark.serializer.KryoSerializer

# PostgreSQL JDBC driver for Spark SQL
spark.jars /opt/bitnami/spark/jars/postgresql-42.3.1.jar
spark.jars.packages org.postgresql:postgresql:42.2.22

# Shuffle service
spark.shuffle.service.enabled true
spark.dynamicAllocation.shuffleTracking.enabled true

# Speculative execution
spark.speculation false

# # Memory fraction
# spark.memory.fraction 0.6
# spark.memory.storageFraction 0.5

# # Network timeout
# spark.network.timeout 120s

# Default parallelism
spark.default.parallelism 4

spark.ui.prometheus.enabled true    

# Hive Integration
spark.sql.warehouse.dir hdfs://namenode:9000/user/hive/warehouse
spark.hadoop.hive.metastore.warehouse.dir hdfs://namenode:9000/user/hive/warehouse
spark.hadoop.hive.metastore.warehouse.external.dir hdfs://namenode:9000/user/hive/warehouse/external
spark.sql.catalogImplementation hive
spark.hadoop.hive.metastore.uris thrift://hive-metastore:9083
spark.sql.hive.metastore.version 2.3.9
spark.sql.hive.metastore.jars path

# Python configuration for YARN
spark.yarn.appMasterEnv.PYSPARK_PYTHON /usr/bin/python3
spark.yarn.appMasterEnv.PYSPARK_DRIVER_PYTHON /usr/bin/python3
spark.executorEnv.PYSPARK_PYTHON /usr/bin/python3