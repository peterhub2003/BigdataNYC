version: "3"

services:
  namenode:
    image: jayachander/hadoop-namenode:1.0.0-hadoop3.3.2-java8
    container_name: namenode
    restart: always
    ports:
      - 9870:9870
      - 9000:9000
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    environment:
      - CLUSTER_NAME=${CLUSTER_NAME}
    env_file:
      - ./hadoop.env
      - ./.env

  datanode1:
    image: jayachander/hadoop-datanode:1.0.0-hadoop3.3.2-java8
    container_name: datanode1
    restart: always
    volumes:
      - hadoop_datanode1:/hadoop/dfs/data
    environment:
      - CLUSTER_NAME=${CLUSTER_NAME}
      - SERVICE_PRECONDITION=${DATANODE_PRECONDITION}
    env_file:
      - ./hadoop.env
      - ./.env

  # Hadoop ResourceManager
  resourcemanager:
    image: jayachander/hadoop-resourcemanager:1.0.0-hadoop3.3.2-java8

    container_name: resourcemanager
    restart: always
    depends_on:
      - namenode
    environment:
      - SERVICE_PRECONDITION=${RESOURCEMANAGER_PRECONDITION}
    env_file:
      - ./hadoop.env
      - ./.env
    ports:
      - "8088:8088" # YARN ResourceManager UI

  # Hadoop NodeManager
  nodemanager1:
    image: thangdoc/hadoop-nodemanager-python:latest
    container_name: nodemanager1
    restart: always
    depends_on:
      - resourcemanager
      - namenode
    environment:
      - SERVICE_PRECONDITION=${NODEMANAGER_PRECONDITION}
    env_file:
      - ./hadoop.env
      - ./.env

  nodemanager2:
    image: thangdoc/hadoop-nodemanager-python:latest
    container_name: nodemanager2
    restart: always
    depends_on:
      - resourcemanager
      - namenode
    environment:
      - SERVICE_PRECONDITION=${NODEMANAGER_PRECONDITION}
    env_file:
      - ./hadoop.env
      - ./.env

  nodemanager3:
    image: thangdoc/hadoop-nodemanager-python:latest
    container_name: nodemanager3 
    restart: always
    depends_on:
      - resourcemanager
      - namenode
    environment:
      - SERVICE_PRECONDITION=${NODEMANAGER_PRECONDITION}
    env_file:
      - ./hadoop.env
      - ./.env

  # PostgreSQL Service
  postgres:
    image: postgres:13
    container_name: postgres
    restart: always
    environment:
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      POSTGRES_MULTIPLE_DATABASES: ${POSTGRES_MULTIPLE_DATABASES}
    env_file:
      - ./.env
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data

  # Apache Hive Service
  hive-server:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-server
    restart: always
    depends_on:
      - namenode
      - postgres
      - hive-metastore
    ports:
      - "10000:10000"  # HiveServer2 Thrift port
      - "10002:10002"  # Hive Web UI
    volumes:
      - ./warehouse:/opt/hive/warehouse
      - ./jars/postgresql-42.3.1.jar:/opt/hive/lib/postgresql-42.3.1.jar
    environment:
      - SERVICE_PRECONDITION=${HIVE_SERVER_PRECONDITION}
    env_file:
      - ./.env

  # Hive Metastore Service
  hive-metastore:
    image: bde2020/hive:2.3.2-postgresql-metastore
    container_name: hive-metastore
    restart: always
    depends_on:
      - namenode
      - postgres
    ports:
      - "9083:9083"  # Metastore Thrift port
    volumes:
      - ./jars/postgresql-42.3.1.jar:/opt/hive/lib/postgresql-42.3.1.jar
    environment:
      - SERVICE_PRECONDITION=${HIVE_METASTORE_PRECONDITION}
    env_file:
      - ./.env
    command: /opt/hive/bin/hive --service metastore

  # Jupyter Notebook Service
  jupyter:
    image: jupyter/pyspark-notebook:spark-3.3.0
    container_name: jupyter
    restart: always
    ports:
      - "8888:8888"
      - "4041:4041"
    depends_on:
      - resourcemanager
      - nodemanager1
      - hive-metastore
      - hive-server
    volumes:
      - ./notebooks:/mnt/notebooks
      - ./ETL:/mnt/scripts
      - ./requirements.txt:/tmp/requirements.txt
      - ./jars/postgresql-42.3.1.jar:/usr/local/spark/jars/postgresql-42.3.1.jar
      - ./conf/yarn-site.xml:/usr/local/spark/conf/yarn-site.xml
      - ./conf/core-site.xml:/usr/local/spark/conf/core-site.xml
      - ./conf/hdfs-site.xml:/usr/local/spark/conf/hdfs-site.xml
      - ./conf/hive-site.xml:/usr/local/spark/conf/hive-site.xml
    environment:
      - SPARK_MASTER=yarn
      - SPARK_SUBMIT_DEPLOYMODE=client
      - JUPYTER_ENABLE_LAB=yes

      - SPARK_DRIVER_HOST=jupyter
      - SPARK_DRIVER_MEMORY=1g
      - SPARK_EXECUTOR_MEMORY=1g
      - SPARK_NETWORK_TIMEOUT=800
      - SPARK_UI_PORT=4041
      - HADOOP_CONF_DIR=/usr/local/spark/conf
      - YARN_CONF_DIR=/usr/local/spark/conf
      - SPARK_OPTS="--master=yarn --deploy-mode=client --driver-java-options=-Dlog4j.logLevel=info"
      - SERVICE_PRECONDITION=hive-metastore:9083 resourcemanager:8088
    env_file:
      - ./.env
      - ./hadoop.env
    command: >
      bash -c "pip install -r /tmp/requirements.txt && \
      start-notebook.sh --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.notebook_dir='/mnt/notebooks'"

  superset_db:
    image: postgres:13
    container_name: superset_db
    restart: always
    environment:
      POSTGRES_DB: superset
      POSTGRES_USER: superset
      POSTGRES_PASSWORD: superset
    volumes:
      - superset_db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U superset -d superset"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Superset
  superset:
    # build: ./superset
    image: thangdoc/superset-bigdata:latest
    container_name: superset
    restart: always
    ports:
      - "8089:8088"
    depends_on:
      superset_db:  
        condition: service_healthy
    env_file:
      - path: superset/.env
    environment:
      SUPERSET_CONFIG_PATH: /app/config/superset_config.py
      # Sử dụng biến môi trường theo format SUPERSET__<section>__<key> sẽ ghi đè config file
      # SUPERSET__SQLALCHEMY_DATABASE_URI: "postgresql+psycopg2://superset:superset@superset_db:5432/superset"
      # # Hoặc dùng biến gốc (ưu tiên thấp hơn format trên)
      # # SQLALCHEMY_DATABASE_URI: "postgresql+psycopg2://superset:superset@superset_db:5432/superset"

      SUPERSET_SECRET_KEY: ${SUPERSET_SECRET_KEY:-"YOUR_OWN_RANDOM_SECRET_KEY_CHANGE_ME"}

      ADMIN_USERNAME: ${ADMIN_USERNAME:-admin}
      ADMIN_EMAIL: ${ADMIN_EMAIL:-admin@superset.com}
      ADMIN_PASSWORD: ${ADMIN_PASSWORD:-admin}
      ADMIN_FIRSTNAME: ${ADMIN_FIRSTNAME:-Superset}
      ADMIN_LASTNAME: ${ADMIN_LASTNAME:-Admin}

      SUPERSET_LOAD_EXAMPLES: "${SUPERSET_LOAD_EXAMPLES:-no}"
      SUPERSET_LOG_LEVEL: "${SUPERSET_LOG_LEVEL:-info}"
      # postgresql://<UserName>:<DBPassword>@<Database Host>/<Database Name>
    volumes:
      - ./superset/entrypoint.sh:/app/docker/entrypoint.sh
      - superset_home:/app/superset_home
      - ./superset/superset_config.py:/app/config/superset_config.py
    entrypoint: ["/app/docker/entrypoint.sh"] 
    command: ["/usr/bin/run-server.sh"]

  # Apache NiFi Service
  nifi:
    image: apache/nifi:1.28.0
    container_name: nifi
    restart: always
    ports:
      - "8443:8443"  # NiFi HTTPS Web UI
    environment:
      - NIFI_WEB_HTTPS_PORT=8443
      - NIFI_WEB_HTTP_PORT=
      - NIFI_CLUSTER_IS_NODE=false
      # - NIFI_SENSITIVE_PROPS_KEY=ctsBtRBKHRAx69EqUghvvgEvjnaLjFEB
      - SINGLE_USER_CREDENTIALS_USERNAME=admin
      - SINGLE_USER_CREDENTIALS_PASSWORD=ctsBtRBKHRAx69EqUghvvgEvjnaLjFEB
      - NIFI_SECURITY_USER_AUTHORIZER=single-user-authorizer
      - NIFI_SECURITY_USER_LOGIN_IDENTITY_PROVIDER=single-user-provider
      - NIFI_JVM_HEAP_INIT=1g
      - NIFI_JVM_HEAP_MAX=4g
      - HADOOP_CONF_DIR=/opt/hadoop/conf
      - SERVICE_PRECONDITION=namenode:9000 datanode1:9864
    env_file:
      - ./.env
    volumes:
      - nifi_conf:/opt/nifi/nifi-current/conf
      - nifi_database_repository:/opt/nifi/nifi-current/database_repository
      - nifi_flowfile_repository:/opt/nifi/nifi-current/flowfile_repository
      - nifi_content_repository:/opt/nifi/nifi-current/content_repository
      - nifi_provenance_repository:/opt/nifi/nifi-current/provenance_repository
      - nifi_state:/opt/nifi/nifi-current/state
      - nifi_logs:/opt/nifi/nifi-current/logs
      - ./conf/core-site.xml:/opt/hadoop/conf/core-site.xml
      - ./conf/hdfs-site.xml:/opt/hadoop/conf/hdfs-site.xml
      # - ./scripts:/opt/nifi/nifi-current/scripts
      # - ./nifi_templates:/opt/nifi/nifi-current/conf/templates
    depends_on:
      - namenode
      - datanode1


volumes:
  hadoop_namenode:
  hadoop_datanode1:
  # hadoop_historyserver:
  
  postgres_data:
  warehouse:
  superset_db_data:
  superset_home:

  nifi_conf:
  nifi_database_repository:
  nifi_flowfile_repository:
  nifi_content_repository:
  nifi_provenance_repository:
  nifi_state:
  nifi_logs:

